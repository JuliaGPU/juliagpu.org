<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta content="index, follow" name="robots">
  <meta name="generator" content="Hugo 0.77.0" />
  <link rel="stylesheet" href="https://juliagpu.org/css/bootstrap.min.css">

  
  <link rel="stylesheet" href="/highlight.css">
  <script src="/highlight.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style>
    .hljs {
      padding: 0;
      background: none;
  }
  </style>

  
  
  
  <title>CUDA.jl 1.1 · JuliaGPU</title>
  <style>
.container {
  max-width: 700px;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
.card-img,
.card-img-top,
.card-img-bottom {
  width: initial;
}
</style>

</head>

  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
    
    
    <ul class="navbar-nav">
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/">
              
                
                <i data-feather="home"></i> 
              
              Home
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item active">
            <a class="nav-link" href="/post/">
              
                
                <i data-feather="file-text"></i> 
              
              Blog
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/learn/">
              
                
                <i data-feather="book-open"></i> 
              
              Learn
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/cuda/">
              
              NVIDIA CUDA
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/rocm/">
              
              AMD ROCm
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/oneapi/">
              
              Intel oneAPI
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/other/">
              
              Other
            </a>
          </li>
        
      
    </ul>
  </nav>
</div>

    <div class="container">
      <main id="main">
        

<h1>CUDA.jl 1.1</h1>



<i data-feather="calendar"></i> <time datetime="2020-07-07">Jul 7, 2020</time>




  <br>
  <i data-feather="edit-2"></i> Tim Besard


<br><br>

    <p>CUDA.jl 1.1 marks the first feature release after merging several CUDA packages into one. It
raises the minimal Julia version to 1.4, and comes with support for the impending 1.5
release.</p>
<h2 id="cudajl-replacing-cuarrayscudanativejl">CUDA.jl replacing CuArrays/CUDAnative.jl</h2>
<p>As <a href="https://discourse.julialang.org/t/psa-cuda-jl-replacing-cuarrays-jl-cudanative-jl-cudadrv-jl-cudaapi-jl-call-for-testing/40205">announced a while
back</a>,
CUDA.jl is now the new package for programming CUDA GPUs in Julia, replacing CuArrays.jl,
CUDAnative.jl, CUDAdrv.jl and CUDAapi.jl. The merged package should be a drop-in
replacement: All existing functionality has been ported, and almost all exported functions
are still there. Applications like Flux.jl or the DiffEq.jl stack are being updated to
support this change.</p>
<h2 id="cuda-11-support">CUDA 11 support</h2>
<p>With CUDA.jl 1.1, we support the upcoming release of the CUDA toolkit. This only applies to
locally-installed versions of the toolkit, i.e., you need to specify
<code>JULIA_CUDA_USE_BINARYBUILDER=false</code> in your environment to pick up the locally-installed
release candidate of the CUDA toolkit. New features, like the third-generation tensor cores
and its extended type support, or any new APIs, are not yet natively supported by Julia
code.</p>
<h2 id="nvidia-management-library-nvml">NVIDIA Management Library (NVML)</h2>
<p>CUDA.jl now integrates with the NVIDIA Management Library, or NVML. With this library, it&rsquo;s
possible to query information about the system, any GPU devices, their topology, etc.:</p>
<pre><code class="language-julia-repl">julia&gt; using CUDA

julia&gt; dev = first(NVML.devices())
CUDA.NVML.Device(Ptr{Nothing} @0x00007f987c7c6e38)

julia&gt; NVML.uuid(dev)
UUID(&quot;b8d5e790-ea4d-f962-e0c3-0448f69f2e23&quot;)

julia&gt; NVML.name(dev)
&quot;Quadro RTX 5000&quot;

julia&gt; NVML.power_usage(dev)
37.863

julia&gt; NVML.energy_consumption(dev)
65330.292
</code></pre>
<h2 id="experimental-texture-support">Experimental: Texture support</h2>
<p>It is now also possible to use the GPU&rsquo;s hardware texture support from Julia, albeit using a
fairly low-level and still experimental API (many thanks to
<a href="https://github.com/cdsousa">@cdsousa</a> for the initial development). As a demo, let&rsquo;s start
with loading a sample image:</p>
<pre><code class="language-julia">julia&gt; using Images, TestImages, ColorTypes, FixedPointNumbers
julia&gt; img = RGBA{N0f8}.(testimage(&quot;lighthouse&quot;))
</code></pre>
<p>We use RGBA since CUDA&rsquo;s texture hardware only supports 1, 2 or 4 channels. This support is
also currently limited to &ldquo;plain&rdquo; types, so let&rsquo;s reinterpret the image:</p>
<pre><code class="language-julia">julia&gt; img′ = reinterpret(NTuple{4,UInt8}, img)
</code></pre>
<p>Now we can upload this image to the array, using the <code>CuTextureArray</code> type for optimized
storage (normal <code>CuArray</code>s are supported too), and bind it to a <code>CuTexture</code> object that we
can pass to a kernel:</p>
<pre><code class="language-julia-repl">julia&gt; texturearray = CuTextureArray(img′)

julia&gt; texture = CuTexture(texturearray; normalized_coordinates=true)
512×768 4-channel CuTexture(::CuTextureArray) with eltype NTuple{4,UInt8}
</code></pre>
<p>Let&rsquo;s write and a kernel that warps this image. Since we specified
<code>normalized_coordinates=true</code>, we index the texture using values in <code>[0,1]</code>:</p>
<pre><code class="language-julia">function warp(dst, texture)
    tid = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    I = CartesianIndices(dst)
    @inbounds if tid &lt;= length(I)
        i,j = Tuple(I[tid])
        u = Float32(i-1) / Float32(size(dst, 1)-1)
        v = Float32(j-1) / Float32(size(dst, 2)-1)
        x = u + 0.02f0 * CUDA.sin(30v)
        y = v + 0.03f0 * CUDA.sin(20u)
        dst[i,j] = texture[x,y]
    end
    return
end
</code></pre>
<p>The size of the output image determines how many elements we need to process. This needs to
be translated to a number of threads and blocks, keeping in mind device and kernel
characteristics. We automate this using the occupancy API:</p>
<pre><code class="language-julia-repl">julia&gt; outimg_d = CuArray{eltype(img′)}(undef, 500, 1000);

julia&gt; function configurator(kernel)
           config = launch_configuration(kernel.fun)

           threads = Base.min(length(outimg_d), config.threads)
           blocks = cld(length(outimg_d), threads)

           return (threads=threads, blocks=blocks)
       end

julia&gt; @cuda config=configurator warp(outimg_d, texture)
</code></pre>
<p>Finally, we fetch and visualize the output:</p>
<pre><code class="language-julia-repl">julia&gt; outimg = Array(outimg_d)

julia&gt; save(&quot;imgwarp.png&quot;, reinterpret(eltype(img), outimg))
</code></pre>


<figure>
	<img src="https://juliagpu.org/2020-07-07-cuda_1.1/imgwarp.png" alt="Warped lighthouse" />
</figure>

<h2 id="minor-features">Minor features</h2>
<p>The test-suite is now parallelized, using up-to <code>JULIA_NUM_THREADS</code> processes:</p>
<pre><code>$ JULIA_NUM_THREADS=4 julia -e 'using Pkg; Pkg.test(&quot;CUDA&quot;);'

                                     |          | ---------------- GPU ---------------- | ---------------- CPU ---------------- |
Test                        (Worker) | Time (s) | GC (s) | GC % | Alloc (MB) | RSS (MB) | GC (s) | GC % | Alloc (MB) | RSS (MB) |
initialization                   (2) |     2.52 |   0.00 |  0.0 |       0.00 |   115.00 |   0.05 |  1.8 |     153.13 |   546.27 |
apiutils                         (4) |     0.55 |   0.00 |  0.0 |       0.00 |   115.00 |   0.02 |  4.0 |      75.86 |   522.36 |
codegen                          (4) |    14.81 |   0.36 |  2.5 |       0.00 |   157.00 |   0.62 |  4.2 |    1592.28 |   675.15 |
...
gpuarrays/mapreduce essentials   (2) |   113.52 |   0.01 |  0.0 |       3.19 |   641.00 |   2.61 |  2.3 |    8232.84 |  2449.35 |
gpuarrays/mapreduce (old tests)  (5) |   138.35 |   0.01 |  0.0 |     130.20 |   507.00 |   2.94 |  2.1 |    8615.15 |  2353.62 |
gpuarrays/mapreduce derivatives  (3) |   180.52 |   0.01 |  0.0 |       3.06 |   229.00 |   3.44 |  1.9 |   12262.67 |  1403.39 |

Test Summary: |  Pass  Broken  Total
  Overall     | 11213       3  11216
    SUCCESS
    Testing CUDA tests passed
</code></pre>
<p>A copy of <code>Base.versioninfo()</code> is available to report on the CUDA toolchain and any devices:</p>
<pre><code class="language-julia-repl">julia&gt; CUDA.versioninfo()
CUDA toolkit 10.2.89, artifact installation
CUDA driver 11.0.0
NVIDIA driver 450.36.6

Libraries:
- CUBLAS: 10.2.2
- CURAND: 10.1.2
- CUFFT: 10.1.2
- CUSOLVER: 10.3.0
- CUSPARSE: 10.3.1
- CUPTI: 12.0.0
- NVML: 11.0.0+450.36.6
- CUDNN: 7.6.5 (for CUDA 10.2.0)
- CUTENSOR: 1.1.0 (for CUDA 10.2.0)

Toolchain:
- Julia: 1.5.0-rc1.0
- LLVM: 9.0.1
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4
- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75

1 device(s):
- Quadro RTX 5000 (sm_75, 14.479 GiB / 15.744 GiB available)
</code></pre>
<p>CUTENSOR artifacts have been upgraded to version 1.1.0.</p>
<p>Benchmarking infrastructure based on the Codespeed project has been set-up at
<a href="https://speed.juliagpu.org/">speed.juliagpu.org</a> to keep track of the performance of
various operations.</p>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Made with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme">Vanilla</a>
  </div>
</footer>

    <script src="https://juliagpu.org/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-154489943-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
