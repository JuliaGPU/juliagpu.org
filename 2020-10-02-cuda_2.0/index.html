<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta content="index, follow" name=robots><meta name=generator content="Hugo 0.75.1"><link rel=stylesheet href=https://juliagpu.org/css/bootstrap.min.css><link rel=stylesheet href=/highlight.css><script src=/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><style>.hljs{padding:0;background:0 0}</style><title>CUDA.jl 2.0 · JuliaGPU</title><style>.container{max-width:700px}#nav-border{border-bottom:1px solid #212529}#main{margin-top:1em;margin-bottom:4em}#home-jumbotron{background-color:inherit}#footer .container{padding:1em 0}#footer a{color:inherit;text-decoration:underline}.font-125{font-size:125%}.tag-btn{margin-bottom:.3em}pre{background-color:#f5f5f5;border:1px solid #ccc;border-radius:4px;padding:16px}pre code{padding:0;font-size:inherit;color:inherit;background-color:transparent;border-radius:0}code{padding:2px 4px;font-size:90%;color:#c7254e;background-color:#f9f2f4;border-radius:4px}img,iframe,embed,video,audio{max-width:100%}.card-img,.card-img-top,.card-img-bottom{width:initial}</style></head><body class="d-flex flex-column h-100"><div id=nav-border class=container><nav class="navbar navbar-expand-lg navbar-light justify-content-center"><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/><i data-feather=home></i>Home</a></li><li class="nav-item active"><a class=nav-link href=/post/><i data-feather=file-text></i>Blog</a></li><li class=nav-item><a class=nav-link href=/learn/><i data-feather=book-open></i>Learn</a></li><li class=nav-item><a class=nav-link href=/cuda/>NVIDIA CUDA</a></li><li class=nav-item><a class=nav-link href=/rocm/>AMD ROCm</a></li><li class=nav-item><a class=nav-link href=/oneapi/>Intel oneAPI</a></li><li class=nav-item><a class=nav-link href=/other/>Other</a></li></ul></nav></div><div class=container><main id=main><h1>CUDA.jl 2.0</h1><i data-feather=calendar></i><time datetime=2020-10-02>Oct 2, 2020</time><br><i data-feather=edit-2></i>Tim Besard<br><br><p>Today we&rsquo;re releasing CUDA.jl 2.0, a breaking release with several new features. Highlights
include initial support for Float16, a switch to CUDA&rsquo;s new stream model, a much-needed
rework of the sparse array support and support for CUDA 11.1.</p><p>The release now requires <strong>Julia 1.5</strong>, and assumes a GPU with <strong>compute capability 5.0</strong> or
higher (although most of the package will still work with an older GPU).</p><h2 id=low--and-mixed-precision-operations>Low- and mixed-precision operations</h2><p>With NVIDIA&rsquo;s latest GPUs featuring more and more low-precision operations,
CUDA.jl <a href=https://github.com/JuliaGPU/CUDA.jl/pull/417>now</a> starts to support
these data types. For example, the CUBLAS wrappers can be used with (B)Float16
inputs (running under <code>JULIA_DEBUG=CUBLAS</code> to illustrate the called methods)
thanks to the <code>cublasGemmEx</code> API call:</p><pre><code class=language-julia-repl>julia&gt; mul!(CUDA.zeros(Float32,2,2),
            cu(rand(Float16,2,2)),
            cu(rand(Float16,2,2)))

I! cuBLAS (v11.0) function cublasStatus_t cublasGemmEx(...) called:
i!  Atype: type=cudaDataType_t; val=CUDA_R_16F(2)
i!  Btype: type=cudaDataType_t; val=CUDA_R_16F(2)
i!  Ctype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_32F(68)

2×2 CuArray{Float32,2}:
 0.481284  0.561241
 1.12923   1.04541
</code></pre><pre><code class=language-julia-repl>julia&gt; using BFloat16s

julia&gt; mul!(CUDA.zeros(BFloat16,2,2),
            cu(BFloat16.(rand(2,2))),
            cu(BFloat16.(rand(2,2))))

I! cuBLAS (v11.0) function cublasStatus_t cublasGemmEx(...) called:
i!  Atype: type=cudaDataType_t; val=CUDA_R_16BF(14)
i!  Btype: type=cudaDataType_t; val=CUDA_R_16BF(14)
i!  Ctype: type=cudaDataType_t; val=CUDA_R_16BF(14)
i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_32F(68)

2×2 CuArray{BFloat16,2}:
 0.300781   0.71875
 0.0163574  0.0241699
</code></pre><p>Alternatively, CUBLAS can be configured to automatically down-cast 32-bit inputs to Float16.
This is <a href=https://github.com/JuliaGPU/CUDA.jl/pull/424>now</a> exposed through a task-local
CUDA.jl math mode:</p><pre><code class=language-julia-repl>julia&gt; CUDA.math_mode!(CUDA.FAST_MATH; precision=:Float16)

julia&gt; mul!(CuArray(zeros(Float32,2,2)),
            CuArray(rand(Float32,2,2)),
            CuArray(rand(Float32,2,2)))

I! cuBLAS (v11.0) function cublasStatus_t cublasGemmEx(...) called:
i!  Atype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  Btype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  Ctype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_32F_FAST_16F(74)

2×2 CuArray{Float32,2}:
 0.175258  0.226159
 0.511893  0.331351
</code></pre><p>As part of these changes, CUDA.jl now defaults to using tensor cores. This may affect
accuracy; use math mode <code>PEDANTIC</code> if you want the old behavior.</p><p>Work is <a href=https://github.com/JuliaGPU/CUDA.jl/issues/391>under way</a> to extend these
capabilities to the rest of CUDA.jl, e.g., the CUDNN wrappers, or the native kernel
programming capabilities.</p><h2 id=new-default-stream-semantics>New default stream semantics</h2><p>In CUDA.jl 2.0 we&rsquo;re <a href=https://github.com/JuliaGPU/CUDA.jl/pull/395>switching</a> to CUDA&rsquo;s
<a href=https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/>simplified stream programming
model</a>.
This simplifies working with multiple streams, and opens up more possibilities for
concurrent execution of GPU operations.</p><h3 id=multi-stream-programming>Multi-stream programming</h3><p>In the old model, the default stream (used by all GPU operations unless specified otherwise)
was a special stream whose commands could not be executed concurrently with commands on
regular, explicitly-created streams. For example, if we interleave kernels executed on a
dedicated stream with ones on the default one, execution was serialized:</p><pre><code class=language-julia>using CUDA

N = 1 &lt;&lt; 20

function kernel(x, n)
    tid = threadIdx().x + (blockIdx().x-1) * blockDim().x
    for i = tid:blockDim().x*gridDim().x:n
        x[i] = CUDA.sqrt(CUDA.pow(3.14159f0, i))
    end
    return
end

num_streams = 8

for i in 1:num_streams
    stream = CuStream()

    data = CuArray{Float32}(undef, N)

    @cuda blocks=1 threads=64 stream=stream kernel(data, N)

    @cuda kernel(data, 0)
end
</code></pre><figure><img src=https://juliagpu.org/2020-10-02-cuda_2.0/multistream_before.png alt="Multi-stream programming (old)"></figure><p>In the new model, default streams are regular streams and commands issued on them can
execute concurrently with those on other streams:</p><figure><img src=https://juliagpu.org/2020-10-02-cuda_2.0/multistream_after.png alt="Multi-stream programming (new)"></figure><h3 id=multi-threading>Multi-threading</h3><p>Another consequence of the new stream model is that each thread gets its own default stream
(accessible as <code>CuStreamPerThread()</code>). Together with Julia&rsquo;s threading capabilities, this
makes it trivial to group independent work in tasks, benefiting from concurrent execution on
the GPU where possible:</p><pre><code class=language-julia>using CUDA

N = 1 &lt;&lt; 20

function kernel(x, n)
    tid = threadIdx().x + (blockIdx().x-1) * blockDim().x
    for i = tid:blockDim().x*gridDim().x:n
        x[i] = CUDA.sqrt(CUDA.pow(3.14159f0, i))
    end
    return
end

Threads.@threads for i in 1:Threads.nthreads()
    data = CuArray{Float32}(undef, N)
    @cuda blocks=1 threads=64 kernel(data, N)
    synchronize(CuDefaultStream())
end
</code></pre><figure><img src=https://juliagpu.org/2020-10-02-cuda_2.0/multithread_after.png alt="Multi-threading (new)"></figure><p>With the old model, execution would have been serialized because the default stream was the
same across threads:</p><figure><img src=https://juliagpu.org/2020-10-02-cuda_2.0/multithread_before.png alt="Multi-threading (old)"></figure><p>Future improvements will make this behavior configurable, such that users can use a
different default stream per task.</p><h2 id=sparse-array-clean-up>Sparse array clean-up</h2><p>As part of CUDA.jl 2.0, the sparse array support <a href=https://github.com/JuliaGPU/CUDA.jl/pull/409>has been
refactored</a>, bringing them in line with other
array types and their expected behavior. For example, the custom <code>switch2</code> methods have been
removed in favor of calls to <code>convert</code> and array constructors:</p><pre><code class=language-julia-repl>julia&gt; using SparseArrays
julia&gt; using CUDA, CUDA.CUSPARSE

julia&gt; CuSparseMatrixCSC(CUDA.rand(2,2))
2×2 CuSparseMatrixCSC{Float32} with 4 stored entries:
  [1, 1]  =  0.124012
  [2, 1]  =  0.791714
  [1, 2]  =  0.487905
  [2, 2]  =  0.752466

julia&gt; CuSparseMatrixCOO(sprand(2,2, 0.5))
2×2 CuSparseMatrixCOO{Float64} with 3 stored entries:
  [1, 1]  =  0.183183
  [2, 1]  =  0.966466
  [2, 2]  =  0.064101

julia&gt; CuSparseMatrixCSR(ans)
2×2 CuSparseMatrixCSR{Float64} with 3 stored entries:
  [1, 1]  =  0.183183
  [2, 1]  =  0.966466
  [2, 2]  =  0.064101
</code></pre><p><a href=https://github.com/JuliaGPU/CUDA.jl/pull/421>Initial support for the COO sparse matrix type</a> has also been added, along with more <a href=https://github.com/JuliaGPU/CUDA.jl/pull/351>better
support for sparse matrix-vector
multiplication</a>.</p><h2 id=support-for-cuda-111>Support for CUDA 11.1</h2><p>This release also features support for the brand-new CUDA 11.1. As there is no compatible
release of CUDNN or CUTENSOR yet, CUDA.jl won&rsquo;t automatically select this version, but you
can force it to by setting the <code>JULIA_CUDA_VERSION</code> environment variable to <code>11.1</code>:</p><pre><code class=language-julia-repl>julia&gt; ENV[&quot;JULIA_CUDA_VERSION&quot;] = &quot;11.1&quot;

julia&gt; using CUDA

julia&gt; CUDA.versioninfo()
CUDA toolkit 11.1.0, artifact installation

Libraries:
- CUDNN: missing
- CUTENSOR: missing
</code></pre><h2 id=minor-changes>Minor changes</h2><p>Many other changes are part of this release:</p><ul><li>Views, reshapes and array reinterpretations <a href=https://github.com/JuliaGPU/CUDA.jl/pull/437>are now
represented</a> by the Base array wrappers,
simplifying the CuArray type definition.</li><li>Various optimizations to <a href=https://github.com/JuliaGPU/CUDA.jl/pull/428>CUFFT</a> and
<a href=https://github.com/JuliaGPU/CUDA.jl/pull/321>CUDNN</a> library wrappers.</li><li><a href=https://github.com/JuliaGPU/CUDA.jl/pull/427>Support</a> for <code>LinearAlgebra.reflect!</code> and
<code>rotate!</code></li><li><a href=https://github.com/JuliaGPU/CUDA.jl/pull/435>Initial support</a> for calling CUDA libraries
with strided inputs</li></ul></main></div><footer id=footer class="mt-auto text-center text-muted"><div class=container>Made with <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme>Vanilla</a></div></footer><script src=https://juliagpu.org/js/feather.min.js></script><script>feather.replace()</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-154489943-1','auto');ga('send','pageview');</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>