<!doctype html><html lang=en class=h-100><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta content="index, follow" name=robots><meta name=generator content="Hugo 0.75.1"><link rel=stylesheet href=https://juliagpu.org/css/bootstrap.min.css><link rel=stylesheet href=/highlight.css><script src=/highlight.js></script><script>hljs.initHighlightingOnLoad();</script><style>.hljs{padding:0;background:0 0}</style><title>CUDA.jl 2.1 Â· JuliaGPU</title><style>.container{max-width:700px}#nav-border{border-bottom:1px solid #212529}#main{margin-top:1em;margin-bottom:4em}#home-jumbotron{background-color:inherit}#footer .container{padding:1em 0}#footer a{color:inherit;text-decoration:underline}.font-125{font-size:125%}.tag-btn{margin-bottom:.3em}pre{background-color:#f5f5f5;border:1px solid #ccc;border-radius:4px;padding:16px}pre code{padding:0;font-size:inherit;color:inherit;background-color:transparent;border-radius:0}code{padding:2px 4px;font-size:90%;color:#c7254e;background-color:#f9f2f4;border-radius:4px}img,iframe,embed,video,audio{max-width:100%}.card-img,.card-img-top,.card-img-bottom{width:initial}</style></head><body class="d-flex flex-column h-100"><div id=nav-border class=container><nav class="navbar navbar-expand-lg navbar-light justify-content-center"><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/><i data-feather=home></i>Home</a></li><li class="nav-item active"><a class=nav-link href=/post/><i data-feather=file-text></i>Blog</a></li><li class=nav-item><a class=nav-link href=/learn/><i data-feather=book-open></i>Learn</a></li><li class=nav-item><a class=nav-link href=/cuda/>NVIDIA CUDA</a></li><li class=nav-item><a class=nav-link href=/rocm/>AMD ROCm</a></li><li class=nav-item><a class=nav-link href=/oneapi/>Intel oneAPI</a></li><li class=nav-item><a class=nav-link href=/other/>Other</a></li></ul></nav></div><div class=container><main id=main><h1>CUDA.jl 2.1</h1><i data-feather=calendar></i><time datetime=2020-10-30>Oct 30, 2020</time><br><i data-feather=edit-2></i>Tim Besard<br><br><p>CUDA.jl v2.1 is a bug-fix release, with one new feature: support for cubic texture
interpolations. The release also partly reverts a change from v2.0: <code>reshape</code>, <code>reinterpret</code>
and contiguous <code>view</code>s now return a <code>CuArray</code> again.</p><h2 id=generalized-texture-interpolations>Generalized texture interpolations</h2><p>CUDA&rsquo;s texture hardware only supports nearest-neighbour and linear interpolation, for other
modes one is required to perform the interpolation by hand. In CUDA.jl v2.1 we are
generalizing the texture interpolation API so that it is possible to use both
hardware-backed and software-implemented interpolation modes in exactly the same way:</p><pre><code class=language-julia># N is the dimensionality (1, 2 or 3)
# T is the element type (needs to be supported by the texture hardware)

# source array
src = rand(T, fill(10, N)...)

# indices we want to interpolate
idx = [tuple(rand(1:0.1:10, N)...) for _ in 1:10]

# upload to the GPU
gpu_src = CuArray(src)
gpu_idx = CuArray(idx)

# interpolate using a texture
gpu_dst = CuArray{T}(undef, size(gpu_idx))
gpu_tex = CuTexture(gpu_src; interpolation=CUDA.NearestNeighbour())
broadcast!(gpu_dst, gpu_idx, Ref(gpu_tex)) do idx, tex
    tex[idx...]
end

# back to the CPU
dst = Array(gpu_dst)
</code></pre><p>Here, we can change the <code>interpolation</code> argument to <code>CuTexture</code> to either <code>NearestNeighbour</code>
or <code>LinearInterpolation</code>, both supported by the hardware, or <code>CubicInterpolation</code> which is
implemented in software (building on the hardware-supported linear interpolation).</p><h2 id=partial-revert-of-array-wrapper-changes>Partial revert of array wrapper changes</h2><p>In CUDA.jl v2.0, we changed the behavior of several important array operations to reuse
available wrappers in Base: <code>reshape</code> started returning a <code>ReshapedArray</code>, <code>view</code> now
returned a <code>SubArray</code>, and <code>reinterpret</code> was reworked to use <code>ReinterpretArray</code>. These
changes were made to ensure maximal compatibility with Base&rsquo;s array type, and to simplify
the implementation in CUDA.jl and GPUArrays.jl.</p><p>However, this change turned out to regress the time to precompile and load CUDA.jl.
Consequently, the change has been reverted, and these wrappers are now implemented as part
of the <code>CuArray</code> type again. Note however that we intend to revisit this change in the
future. It is therefore recommended to use the <code>DenseCuArray</code> type alias for methods that
need a <code>CuArray</code> backed by contiguous GPU memory. For strided <code>CuArray</code>s, i.e.
non-contiguous views, you should use the <code>StridedCuArray</code> alias.</p></main></div><footer id=footer class="mt-auto text-center text-muted"><div class=container>Made with <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme>Vanilla</a></div></footer><script src=https://juliagpu.org/js/feather.min.js></script><script>feather.replace()</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-154489943-1','auto');ga('send','pageview');</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>