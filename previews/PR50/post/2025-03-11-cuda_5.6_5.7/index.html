<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR50/assets/favicon.ico">
  <link rel="alternate" type="application/rss+xml" href="/previews/PR50/post/index.xml" title="RSS Feed for JuliaGPU">

  <link rel="stylesheet" href="/previews/PR50/css/bootstrap.min.css">
  
   <link rel="stylesheet" href="/previews/PR50/libs/highlight/github.min.css">
 

  <style>
 .hljs {
     padding: 0;
     background: 0 0
 }
.container {
   max-width: 700px
}

#nav-border {
   border-bottom: 1px solid #212529
}

#main {
   margin-top: 1em;
   margin-bottom: 4em
}

#home-jumbotron {
   background-color: inherit
}

#footer .container {
   padding: 1em 0
}

#footer a {
   color: inherit;
   text-decoration: underline
}

.font-125 {
   font-size: 125%
}

.tag-btn {
   margin-bottom: .3em
}

pre {
   background-color: #f5f5f5;
   border: 1px solid #ccc;
   border-radius: 4px;
   padding: 16px
}

pre code {
   padding: 0;
   font-size: inherit;
   color: inherit;
   background-color: transparent;
   border-radius: 0
}

code {
   padding: 2px 4px;
   font-size: 90%;
   color: #c7254e;
   background-color: #f9f2f4;
   border-radius: 4px
}

img,
iframe,
embed,
video,
audio {
   max-width: 100%
}

.card-img,
.card-img-top,
.card-img-bottom {
   width: initial
}

#main h1>a, #main h2>a, #main h3>a {
   color: inherit;
   text-decoration: none;
}

li p {
   margin: 0
}

</style>


  
  
    <title>CUDA.jl 5.6 and 5.7: Allocator cache, and asynchronous CUBLAS wrappers ⋅ JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
        <ul class=navbar-nav>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/"><i data-feather=home></i>Home</a>
            </li>
            <li class="nav-item active"><a class=nav-link href="/previews/PR50/post/"><i data-feather=file-text></i>Blog</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/learn/"><i data-feather=book-open></i>Learn</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/cuda/">CUDA</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/rocm/">ROCm</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/oneapi/">oneAPI</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/metal/">Metal</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR50/other/">Other</a>
            </li>
        </ul>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
      <!-- make sure to generate a Hugo-era URI at the root, redirecting to /post/.... -->
      

      <h1>CUDA.jl 5.6 and 5.7: Allocator cache, and asynchronous CUBLAS wrappers</h1>
      <i data-feather=calendar></i>
<time datetime=2025-3-11>Mar 11, 2025</time><br>
<i data-feather=edit-2></i>
Tim Besard


      <br><br>
    
<!-- Content appended here -->

<p>CUDA.jl v5.6 adds support for the new GPUArrays.jl caching allocator interface, which should improve performance of repetitive, memory-heavy allocations. CUDA.jl v5.7 brings a greatly improved <code>CuRef</code> type, which enables fully asynchronous CUBLAS calls.</p>
<h2 id="reworking_curef_for_asynchronous_cublas"><a href="#reworking_curef_for_asynchronous_cublas" class="header-anchor">Reworking <code>CuRef</code> for asynchronous CUBLAS</a></h2>
<p>The <code>CuRef</code> type is similar to Julia&#39;s <code>Ref</code>, a boxed value, often used with C APIs. In CUDA.jl v5.7, we&#39;ve made several changes to this type. First of all, we&#39;ve aligned its API much more closely with the <code>Ref</code> type from Base, e.g, adding <code>getindex</code> and <code>setindex&#33;</code> methods, which should make it more familiar to users:</p>
<pre><code class="language-julia-repl">julia&gt; box &#61; CuRef&#40;1&#41;
CuRefValue&#123;Int64&#125;&#40;1&#41;

julia&gt; box&#91;&#93;
1

julia&gt; box&#91;&#93; &#61; 2
2

julia&gt; box
CuRefValue&#123;Int64&#125;&#40;2&#41;</code></pre>
<p>We also <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2645">optimized and improved</a> the <code>CuRef</code> implementation. As part of that work, we <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2625">removed the eager synchronization when copying from unpinned memory</a>. This was done to make it possible for Julia code to execute when waiting for the memory copy to start. However, it turns out that certain &#40;small&#41; copies, such as those performed by <code>CuRef</code>, can be performed without having to wait for the copy to start. By removing eager synchronization from those copies, <code>CuRef</code> objects can now be constructed fully asynchronously, i.e., without having to wait for the GPU to be ready.</p>
<p>Building on these changes, <a href="https://github.com/kshyatt">@kshyatt</a> has <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2616">switched our CUBLAS wrappers</a> over to using GPU-based <code>CuRef</code> boxes for scalar inputs instead of host-based <code>Ref</code> boxes. Although this increases the complexity of invoking CUBLAS APIs – the allocation of <code>CuRef</code> boxes requires CUDA API calls whereas a <code>Ref</code> box is much cheaper to allocate – this results in the API behaving asynchronously, whereas before every CUBLAS API taking scalar inputs would have resulted in a so-called &quot;bubble&quot; waiting for the GPU to finish executing.</p>
<h2 id="a_julia-level_allocator_cache"><a href="#a_julia-level_allocator_cache" class="header-anchor">A Julia-level allocator cache</a></h2>
<p>To help with the common issue of running out of GPU memory, or to reduce the cost of CUDA.jl hitting the GC too often, <a href="https://github.com/pxl-th">@pxl-th</a> <a href="https://github.com/JuliaGPU/GPUArrays.jl/pull/576">has added a reusable caching allocator</a> to GPUArrays.jl, which CUDA.jl now supports and integrates with.</p>
<p>The idea is simple: GPU allocations made in a <code>GPUArrays.@cached</code> block are recorded in a <code>cache</code>, and when the block is exited the allocations are made available for reuse. Only when the cache goes out of scope, or when you call <code>unsafe_free&#33;</code> on it, the allocations will be fully freed. This is useful when you have a repetitive workload that performs the same allocations over and over again, such as in a machine learning training loop:</p>
<pre><code class="language-julia">cache &#61; GPUArrays.AllocCache&#40;&#41;
for epoch in 1:1000
    GPUArrays.@cached cache begin
        # dummy workload
        sin.&#40;CUDA.rand&#40;Float32, 1024^3&#41;&#41;
    end
end

# wait for &#96;cache&#96; to be collected, or optionally eagerly free the memory
GPUArrays.unsafe_free&#33;&#40;cache&#41;</code></pre>
<p>Even though CUDA already has a caching allocator, the Julia-level caching mechanism may still improve performance by lowering pressure on the GC and reducing fragmentation of the underlying allocator. For example, the above snippet only performs two memory allocations that require 8 GiB, instead of 2000 allocations totalling 8 TiB &#40;&#33;&#41; of GPU memory.</p>
<p>The cherry on top is that the caching interface is generic, implemented in GPUArrays.jl, and available to all GPU back-ends that are compatible with v11.2.</p>
<h2 id="minor_changes"><a href="#minor_changes" class="header-anchor">Minor changes</a></h2>
<ul>
<li><p>Device-to-host copies <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2648">now eagerly synchronize</a> to improve concurrent execution.</p>
</li>
<li><p>On multi-GPU systems, unified memory <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2626">is not automatically prefetched anymore</a> when launching kernels, making it possible to process a single array on multiple devices.</p>
</li>
<li><p>A change to <code>CuDeviceArray</code> <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2621">should allow eliding additional bounds checks</a> in code that already performs a manual bounds check &#40;such as KernelAbstractions.jl code&#41;</p>
</li>
<li><p>CUDA toolkit 12.8 <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2634">is now supported</a>, <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2620">as well as Jetson Orin</a> devices.</p>
</li>
<li><p>It is now possible to <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2624">pass symbols to kernels</a>.</p>
</li>
<li><p>CUBLAS: <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2642">Support for Givens rotation methods</a>.</p>
</li>
<li><p>CUSPARSE: Support for <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2639">using CuSparseMatrixBSR with generic <code>mm&#33;</code></a>.</p>
</li>
<li><p>Windows support for NVTX <a href="https://github.com/JuliaGPU/CUDA.jl/pull/2665">has been fixed</a>.</p>
</li>
</ul>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->


    
    
        <script src="/previews/PR50/libs/highlight/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="/previews/PR50/libs/feather/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
