<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR52/assets/favicon.ico">
  <link rel="alternate" type="application/rss+xml" href="/previews/PR52/post/index.xml" title="RSS Feed for JuliaGPU">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
   <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/default.min.css">
 

  <link rel="stylesheet" href="/previews/PR52/css/style.css">

  
  
    <title>Metal.jl 0.2: Metal Performance Shaders ⋅ JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-sm navbar-light">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle
navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
            <ul class=navbar-nav>
                <li class="nav-item ">
                    <a class=nav-link href="/previews/PR52/"><i data-feather=home></i> Home</a>
                </li>
                <li class="nav-item active">
                    <a class=nav-link href="/previews/PR52/post/"><i data-feather=file-text></i> News</a>
                </li>
                <li class="nav-item ">
                    <a class=nav-link href="/previews/PR52/showcases/"><i data-feather=star></i> Showcases</a>
                </li>
                <li class="nav-item ">
                    <a class=nav-link href="/previews/PR52/learn/"><i data-feather=book-open></i> Learn</a>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle " href="#" id="backendDropdown"
                        role="button" data-bs-toggle="dropdown" aria-expanded="false">
                        <i data-feather=cpu></i> Backends
                    </a>
                    <div class="dropdown-menu" aria-labelledby="backendDropdown">
                        <a class="dropdown-item " href="/previews/PR52/backends/cuda/">CUDA</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/rocm/">ROCm</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/oneapi/">oneAPI</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/metal/">Metal</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/opencl/">OpenCL</a>
                    </div>
                </li>
            </ul>
        </div>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
      <!-- make sure to generate a Hugo-era URI at the root, redirecting to /post/.... -->
      

      <h1>Metal.jl 0.2: Metal Performance Shaders</h1>
      <i data-feather=calendar></i>
<time datetime=2023-3-3>Mar 3, 2023</time><br>
<i data-feather=edit-2></i>
Tim Besard


      <br><br>
    
<!-- Content appended here -->

<p>Metal.jl 0.2 marks a significant milestone in the development of the Metal.jl package. The release comes with initial support for the Metal Perform Shaders &#40;MPS&#41; framework for accelerating common operations like matrix multiplications, as well as various improvements for writing Metal kernels in Julia.</p>
<h2 id="metal_performance_shaders"><a href="#metal_performance_shaders" class="header-anchor">Metal Performance Shaders</a></h2>
<p>Quoting the <a href="https://developer.apple.com/documentation/metalperformanceshaders">Apple documentation</a>, The Metal Performance Shaders &#40;MPS&#41; framework contains a collection of highly optimized compute and graphics shaders for use in Metal applications. With Metal.jl 0.2, we have added initial support for this framework, and used it to accelerate the matrix multiplication operation:</p>
<pre><code class="language-julia-repl">julia&gt; using Metal, LinearAlgebra, BenchmarkTools
julia&gt; n &#61; p &#61; m &#61; 2048
julia&gt; flops &#61; n*m*&#40;2p-1&#41;
17175674880

julia&gt; a &#61; MtlArray&#40;rand&#40;Float32, n, p&#41;&#41;;
julia&gt; b &#61; MtlArray&#40;rand&#40;Float32, p, m&#41;&#41;;
julia&gt; c &#61; MtlArray&#40;zeros&#40;Float32, n, m&#41;&#41;;

julia&gt; using LinearAlgebra
julia&gt; bench &#61; @benchmark Metal.@sync mul&#33;&#40;c, a, b&#41;
BenchmarkTools.Trial: 518 samples with 1 evaluation.
 Range &#40;min … max&#41;:  9.366 ms …  13.354 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00&#37;
 Time  &#40;median&#41;:     9.629 ms               ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   9.646 ms ± 192.169 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00&#37;

               ▃▂▅▅▆▆▆▇█▇▇▆▅▄▄▁▁ ▁
  ▄▁▄▄▄▄▆▆▆▄▄▁▇█████████████████▄█▄▁▆▁▄▁▆▁▇▁▄▄▁▁▄▄▇▁▄▆▄▁▁▁▁▁▄ █
  9.37 ms      Histogram: log&#40;frequency&#41; by time      10.1 ms &lt;

 Memory estimate: 352 bytes, allocs estimate: 12.

julia&gt; flops / &#40;minimum&#40;bench.times&#41;/1e9&#41;
1.83e12</code></pre>
<p>The benchmark above shows that on an 8-core M1 Pro matrix multiplication now reaches 1.8 TFLOPS &#40;out of the 2.6TFLOPS of theoretical performance&#41;. The accelerated matrix multiplication is available for a variety of input types, incuding mixed-mode operations, and as shown above is integrated with the LinearAlgebra.jl <code>mul&#33;</code> interface.</p>
<p>Of course, the MPS framework offers more than just matrix multiplication, and we expect to support more of it in the future. If you have a specific operation you would like to use from Julia, please let us know by opening an issue on the Metal.jl repository.</p>
<h2 id="gpu_profiling_support"><a href="#gpu_profiling_support" class="header-anchor">GPU profiling support</a></h2>
<p>To support the development of Metal kernels, <a href="https://github.com/max-Hawkins">Max Hawkins</a> has added support for GPU profiling. Similar to how this works in CUDA.jl, you can run code under the <code>Metal.@profile</code> macro to record its execution. However, this does first require setting the <code>METAL_CAPTURE_ENABLED</code> environment flag <em>before</em> import Metal.jl:</p>
<pre><code class="language-julia-repl">julia&gt; ENV&#91;&quot;METAL_CAPTURE_ENABLED&quot;&#93; &#61; 1

julia&gt; using Metal

julia&gt; a &#61; mtl&#40;rand&#40;1024, 1024&#41;&#41;
julia&gt; Metal.@profile sum&#40;a&#41;
&#91; Info: GPU frame capture saved to jl_metal.gputrace/</code></pre>
<p>The resulting capture can be opened with Xcode, presenting a timeline that&#39;s similar to other profilers:</p>
<figure>
  <img src="/previews/PR52/post/2023-03-03-metal_0.2/xcode.png" alt="XCode viewing a Metal.jl capture trace">
</figure>

<h2 id="other_improvements"><a href="#other_improvements" class="header-anchor">Other improvements</a></h2>
<ul>
<li><p>Julia 1.9 is supported, but requires an up-to-date macOS version &#40;issues have been encountered on macOS 12.4&#41;;</p>
</li>
<li><p>An <code>mtl</code> function has been added for converting Julia arrays to Metal arrays, similar to the <code>cu</code> function in CUDA.jl;</p>
</li>
<li><p>Multiple GPUs are supported, and the <code>device&#33;</code> function can be used to select one;</p>
</li>
<li><p>Coverage for SIMD Group functions has been improved, so it&#39;s is now possible to use <code>simdgroup_load</code>, <code>simdgroup_store</code>, <code>simdgroup_multiply</code>, and <code>simdgroup_multiply_accumulate</code> in kernels functions.</p>
</li>
</ul>
<h2 id="future_work"><a href="#future_work" class="header-anchor">Future work</a></h2>
<p>Although Metal.jl is now usable for a variety of applications, there is still work to be done before it can be considered production-ready. In particular:</p>
<ul>
<li><p>there are known performance issues with <code>mapreduce</code>, and other operations that realy on <code>CartesianIndices</code>;</p>
</li>
<li><p>the <code>libcmt</code> wrapper library for interfacing with the Metal APIs is cumbersome to use and improve, and we are looking into native ObjectiveC FFI instead;</p>
</li>
<li><p>the MPS wrappers are incomplete, and similar to the Metal APIs requires a replacement to <code>libcmt</code> to be improved;</p>
</li>
<li><p>support for atomic operations is missing, which is required to implement a full-featured KernelAbstractions.jl back-end.</p>
</li>
</ul>
<p>Once &#40;most of&#41; these issues are addressed, we should be able to release Metal.jl 1.0.</p>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->

    
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"></script>

<!-- Languages -->
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/julia.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/julia-repl.min.js"></script>

<script>hljs.highlightAll();</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="https://cdn.jsdelivr.net/npm/feather-icons@4.29.2/dist/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- BOOTSTRAP -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/js/bootstrap.bundle.min.js"></script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
