<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR52/assets/favicon.ico">
  <link rel="alternate" type="application/rss+xml" href="/previews/PR52/post/index.xml" title="RSS Feed for JuliaGPU">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
   <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/default.min.css">
 

  <link rel="stylesheet" href="/previews/PR52/css/style.css">

  
  
    <title>Showcases ⋅ JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-sm navbar-light">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle
navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
            <ul class=navbar-nav>
                <li class="nav-item ">
                    <a class=nav-link href="/previews/PR52/"><i data-feather=home></i> Home</a>
                </li>
                <li class="nav-item ">
                    <a class=nav-link href="/previews/PR52/post/"><i data-feather=file-text></i> News</a>
                </li>
                <li class="nav-item active">
                    <a class=nav-link href="/previews/PR52/showcases/"><i data-feather=star></i> Showcases</a>
                </li>
                <li class="nav-item ">
                    <a class=nav-link href="/previews/PR52/learn/"><i data-feather=book-open></i> Learn</a>
                </li>
                <li class="nav-item dropdown">
                    <a class="nav-link dropdown-toggle " href="#" id="backendDropdown"
                        role="button" data-bs-toggle="dropdown" aria-expanded="false">
                        <i data-feather=cpu></i> Backends
                    </a>
                    <div class="dropdown-menu" aria-labelledby="backendDropdown">
                        <a class="dropdown-item " href="/previews/PR52/backends/cuda/">CUDA</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/rocm/">ROCm</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/oneapi/">oneAPI</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/metal/">Metal</a>
                        <a class="dropdown-item " href="/previews/PR52/backends/opencl/">OpenCL</a>
                    </div>
                </li>
            </ul>
        </div>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
<!-- Content appended here -->

<h1 id="showcases"><a href="#showcases" class="header-anchor">Showcases</a></h1>
<p>The Julia language, combined with its powerful GPU ecosystem, enables developers and researchers to tackle demanding computational problems with unprecedented productivity and performance. With <strong>over <a href="https://juliahub.com/ui/Packages/General/GPUArrays#dependents">600 dependent packages</a></strong>, it is clear that Julia&#39;s GPU capabilities are not confined to a few niche areas; they are a foundational element empowering a vast and diverse scientific and engineering ecosystem:</p>
<ul>
<li><p><strong>Climate, Ocean, and Earth Sciences</strong>: <a href="https://juliahub.com/ui/Packages/General/Oceananigans">Oceananigans.jl</a>, <a href="https://juliahub.com/ui/Packages/General/GeophysicalFlows">GeophysicalFlows.jl</a>, <a href="https://juliahub.com/ui/Packages/General/MagmaThermoKinematics">MagmaThermoKinematics.jl</a>, <a href="https://juliahub.com/ui/Packages/General/PlanktonIndividuals">PlanktonIndividuals.jl</a>, <a href="https://juliahub.com/ui/Packages/General/RRTMGP">RRTMGP.jl</a>, <a href="https://juliahub.com/ui/Packages/General/SeisNoise">SeisNoise.jl</a>, <a href="https://juliahub.com/ui/Packages/General/vSmartMOM">vSmartMOM.jl</a></p>
</li>
<li><p><strong>Physics &amp; Engineering Simulation</strong>: <a href="https://juliahub.com/ui/Packages/General/MicroMagnetic">MicroMagnetic.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Swalbe">Swalbe.jl</a>, <a href="https://juliahub.com/ui/Packages/General/ParticleHolography">ParticleHolography.jl</a>, <a href="https://juliahub.com/ui/Packages/General/MHDFlows">MHDFlows.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Molly">Molly.jl</a>, <a href="https://juliahub.com/ui/Packages/General/WaterLily">WaterLily.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Tortuosity">Tortuosity.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Trixi">Trixi.jl</a></p>
</li>
<li><p><strong>Power Systems &amp; Energy</strong>: <a href="https://juliahub.com/ui/Packages/General/ExaPF">ExaPF.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Argos">Argos.jl</a>, <a href="https://juliahub.com/ui/Packages/General/ProxAL">ProxAL.jl</a></p>
</li>
<li><p><strong>Machine Learning &amp; Artificial Intelligence</strong>: <a href="https://juliahub.com/ui/Packages/General/Flux">Flux.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Lux">Lux.jl</a>, <a href="https://juliahub.com/ui/Packages/General/MeshGraphNets">MeshGraphNets.jl</a>, <a href="https://juliahub.com/ui/Packages/General/ObjectDetector">ObjectDetector.jl</a>, <a href="https://juliahub.com/ui/Packages/General/LogicCircuits">LogicCircuits.jl</a></p>
</li>
<li><p><strong>Quantum Science &amp; Technology</strong>: <a href="https://juliahub.com/ui/Packages/General/Yao">Yao.jl</a>, <a href="https://juliahub.com/ui/Packages/General/ITensor">ITensor.jl</a>, <a href="https://juliahub.com/ui/Packages/General/TensorOperations">TensorOperations.jl</a></p>
</li>
<li><p><strong>Biomedical &amp; Medical Physics</strong>: <a href="https://juliahub.com/ui/Packages/General/KomaMRI">KomaMRI.jl</a>, <a href="https://juliahub.com/ui/Packages/General/NeuroAnalyzer">NeuroAnalyzer.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Unfold">Unfold.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Roentgen">Roentgen.jl</a></p>
</li>
<li><p><strong>Bioinformatics &amp; Computational Biology</strong>: <a href="https://juliahub.com/ui/Packages/General/Ebic">Ebic.jl</a>, <a href="https://juliahub.com/ui/Packages/General/SimSpread">SimSpread.jl</a></p>
</li>
<li><p><strong>Numerical &amp; Data Analysis</strong>: <a href="https://juliahub.com/ui/Packages/General/MonteCarloMeasurements">MonteCarloMeasurements.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Krylov">Krylov.jl</a>, <a href="https://juliahub.com/ui/Packages/General/MadNLP">MadNLP.jl</a>, <a href="https://juliahub.com/ui/Packages/General/NextLA">NextLA.jl</a>, <a href="https://juliahub.com/ui/Packages/General/DistStat">DistStat.jl</a>, <a href="https://juliahub.com/ui/Packages/General/DifferentialEquations">DifferentialEquations.jl</a></p>
</li>
<li><p><strong>High-Performance Computing</strong>: <a href="https://juliahub.com/ui/Packages/General/ParallelStencil">ParallelStencil.jl</a>, <a href="https://juliahub.com/ui/Packages/General/Dagger">Dagger.jl</a></p>
</li>
</ul>
<p>This diverse adoption underscores the power and flexibility of Julia&#39;s GPU tools. Now, let&#39;s dive into a few noteworthy examples in more detail:</p>
<h2 id="oceananigansjl_simulating_fluid_dynamics"><a href="#oceananigansjl_simulating_fluid_dynamics" class="header-anchor">Oceananigans.jl: Simulating Fluid Dynamics</a></h2>
<p><strong><a href="https://github.com/CliMA/Oceananigans.jl">Oceananigans.jl</a></strong> is a fast, friendly, and flexible Julia package for the numerical simulation of incompressible, stratified, rotating fluid flows on CPUs and GPUs. Developed as part of the Climate Modeling Alliance &#40;CliMA&#41;, it&#39;s designed for both cutting-edge research into small-scale ocean physics and for educational purposes.</p>

<div class="card mb-3">
  <a href="/previews/PR52/assets/img/oceananigans-simulation.png">
    <img src="/previews/PR52/assets/img/oceananigans-simulation.png" class=card-img-top alt>
  </a>
  <div class=card-body>
    <p class=card-text align=center>
      Vertical vorticity as simulated by Oceananigans after a one year integration.
    </p>
  </div>
</div>

<p>Leveraging Julia and <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a>, Oceananigans.jl achieves significant GPU speedups, often <strong>~3x more cost-effective</strong> than CPU simulations and capable of handling <strong>~150 million grid points</strong> on a single high-end GPU. These performance gains permit the long-time integration of realistic simulations, such as large eddy simulation of oceanic boundary layer turbulence over a seasonal cycle or the generation of training data for turbulence parameterizations in Earth system models</p>

<div class="card mb-3">
  <a href="/previews/PR52/assets/img/oceananigans-energy.png">
    <img src="/previews/PR52/assets/img/oceananigans-energy.png" class=card-img-top alt>
  </a>
  <div class=card-body>
    <p class=card-text align=center>
      Simulated years computed by a megawatt-hour of energy (SWPMWh) versus <br />
      number of grid points for state-of-the-art atmosphere and ocean models.
    </p>
  </div>
</div>

<p>GPU support in Oceananigans.jl is built on top of KernelAbstractions.jl, and simply requires the user to specify the <code>GPU&#40;&#41;</code> backend when creating the grid:</p>
<pre><code class="language-julia">using Oceananigans

grid &#61; RectilinearGrid&#40;GPU&#40;&#41;, size&#61;&#40;128, 128&#41;, x&#61;&#40;0, 2π&#41;, y&#61;&#40;0, 2π&#41;,
                       topology&#61;&#40;Periodic, Periodic, Flat&#41;&#41;
model &#61; NonhydrostaticModel&#40;; grid, advection&#61;WENO&#40;&#41;&#41;
ϵ&#40;x, y&#41; &#61; 2rand&#40;&#41; - 1
set&#33;&#40;model, u&#61;ϵ, v&#61;ϵ&#41;
simulation &#61; Simulation&#40;model; Δt&#61;0.01, stop_time&#61;4&#41;
run&#33;&#40;simulation&#41;</code></pre>
<h2 id="diffeqgpujl_accelerating_scientific_simulation"><a href="#diffeqgpujl_accelerating_scientific_simulation" class="header-anchor">DiffEqGPU.jl: Accelerating Scientific Simulation</a></h2>
<p>Part of the acclaimed <strong><a href="https://sciml.ai/">SciML</a></strong> ecosystem, <strong><a href="https://github.com/SciML/DiffEqGPU.jl">DiffEqGPU.jl</a></strong> significantly accelerates the solution of large ensembles of differential equations &#40;ODEs, SDEs&#41;. It provides highly optimized GPU kernels that enable substantial speedups for tasks like parameter sweeps, uncertainty quantification, and simulating complex systems in various scientific fields—often without requiring users to modify their existing model code.</p>
<p>DiffEqGPU.jl achieves high, vendor-agnostic performance. Benchmarks show it can outperform hand-optimized C&#43;&#43;/CUDA solutions and run considerably faster &#40;e.g., <strong>20-100x</strong>&#41; than <code>vmap</code>-based approaches in frameworks like JAX or PyTorch. This capability extends across NVIDIA, AMD, Intel, and Apple GPUs, largely due to its foundation on KernelAbstractions.jl.</p>

<div class="card mb-3">
  <a href="/previews/PR52/assets/img/diffeqgpu-lorenz.jpg">
    <img src="/previews/PR52/assets/img/diffeqgpu-lorenz.jpg" class=card-img-top alt>
  </a>
  <div class=card-body>
    <p class=card-text align=center>
      Solution time for an adaptive Lorenz ODE versus the number of trajectories.
    </p>
  </div>
</div>

<p>The code changes required to solve this classical problem on the GPU are minimal. After setting up the ensemble of Lorenz equations, one can simply replace the <code>EnsembleThreads&#40;&#41;</code> method with <code>EnsembleGPUArray&#40;&#41;</code> to run the simulation on the GPU:</p>
<pre><code class="language-julia">using DiffEqGPU, OrdinaryDiffEq, CUDA

function lorenz&#40;du, u, p, t&#41;
    du&#91;1&#93; &#61; p&#91;1&#93; * &#40;u&#91;2&#93; - u&#91;1&#93;&#41;
    du&#91;2&#93; &#61; u&#91;1&#93; * &#40;p&#91;2&#93; - u&#91;3&#93;&#41; - u&#91;2&#93;
    du&#91;3&#93; &#61; u&#91;1&#93; * u&#91;2&#93; - p&#91;3&#93; * u&#91;3&#93;
end

u0 &#61; Float32&#91;1.0; 0.0; 0.0&#93;
tspan &#61; &#40;0.0f0, 100.0f0&#41;
p &#61; &#91;10.0f0, 28.0f0, 8 / 3.0f0&#93;
prob &#61; ODEProblem&#40;lorenz, u0, tspan, p&#41;
prob_func &#61; function &#40;prob, i, repeat&#41;
    remake&#40;prob, p &#61; rand&#40;Float32, 3&#41; .* p&#41;
end
monteprob &#61; EnsembleProblem&#40;prob, prob_func &#61; prob_func, safetycopy &#61; false&#41;

# CPU-based
sol &#61; solve&#40;monteprob, Tsit5&#40;&#41;, EnsembleThreads&#40;&#41;,
            trajectories &#61; 10_000, saveat &#61; 1.0f0&#41;;

# GPU-based
sol &#61; solve&#40;monteprob, Tsit5&#40;&#41;, EnsembleGPUArray&#40;CUDA.CUDABackend&#40;&#41;&#41;,
            trajectories &#61; 10_000, saveat &#61; 1.0f0&#41;;</code></pre>
<p>The use of <code>EnsembleGPUArray</code> has a bit of overhead because it parallelizes at the level of array operations. It is also possible to execute the entire solver on the GPU by using <code>EnsembleGPUKernel</code>, which offers even better performance, albeit at the the cost of some flexibility &#40;such as using special solvers, and writing the problem in out-of-place form&#41;:</p>
<pre><code class="language-julia">using DiffEqGPU, OrdinaryDiffEq, StaticArrays, CUDA

function lorenz2&#40;u, p, t&#41;
    σ &#61; p&#91;1&#93;
    ρ &#61; p&#91;2&#93;
    β &#61; p&#91;3&#93;
    du1 &#61; σ * &#40;u&#91;2&#93; - u&#91;1&#93;&#41;
    du2 &#61; u&#91;1&#93; * &#40;ρ - u&#91;3&#93;&#41; - u&#91;2&#93;
    du3 &#61; u&#91;1&#93; * u&#91;2&#93; - β * u&#91;3&#93;
    return SVector&#123;3&#125;&#40;du1, du2, du3&#41;
end

u0 &#61; @SVector &#91;1.0f0; 0.0f0; 0.0f0&#93;
tspan &#61; &#40;0.0f0, 10.0f0&#41;
p &#61; @SVector &#91;10.0f0, 28.0f0, 8 / 3.0f0&#93;
prob &#61; ODEProblem&#123;false&#125;&#40;lorenz2, u0, tspan, p&#41;
prob_func &#61; function &#40;prob, i, repeat&#41;
    remake&#40;prob, p &#61; &#40;@SVector rand&#40;Float32, 3&#41;&#41; .* p&#41;
end
monteprob &#61; EnsembleProblem&#40;prob, prob_func &#61; prob_func, safetycopy &#61; false&#41;

sol &#61; solve&#40;monteprob, GPUTsit5&#40;&#41;, EnsembleGPUKernel&#40;CUDA.CUDABackend&#40;&#41;&#41;,
            trajectories &#61; 10_000, saveat &#61; 1.0f0&#41;</code></pre>
<h2 id="fluxjl_elegant_machine_learning"><a href="#fluxjl_elegant_machine_learning" class="header-anchor">Flux.jl: Elegant Machine Learning</a></h2>
<p><strong><a href="https://github.com/FluxML/Flux.jl">Flux.jl</a></strong> is Julia&#39;s premier machine learning library, known for its <strong>100&#37; pure-Julia stack</strong> and lightweight abstractions. Flux is designed for flexibility, extensibility, and seamless integration with the broader Julia ecosystem. The package excels in allowing developers to easily express complex models, write custom layers or training loops, and leverage GPU acceleration with remarkable simplicity.</p>
<p>Here&#39;s an example of a simple multi-layer perceptron &#40;MLP&#41; model defined and trained on a synthetic dataset. The data is batched and subsequently moved to the GPU by using the <code>device</code> function, requiring minimal code changes:</p>
<pre><code class="language-julia">using Flux
using CUDA # Or AMDGPU, Metal

# Function to move data and model to the GPU
device &#61; gpu_device&#40;&#41;

# Define our model, and upload it to the GPU
model &#61; Chain&#40;
    # multi-layer perceptron with one hidden layer of size 3
    Dense&#40;2 &#61;&gt; 3, tanh&#41;,
    BatchNorm&#40;3&#41;,
    Dense&#40;3 &#61;&gt; 2&#41;&#41; |&gt; device

# Generate some data on the CPU
noisy &#61; rand&#40;Float32, 2, 1000&#41;
truth &#61; &#91;xor&#40;col&#91;1&#93;&gt;0.5, col&#91;2&#93;&gt;0.5&#41; for col in eachcol&#40;noisy&#41;&#93;

# Training loop &#40;processing the data in batches&#41;
target &#61; Flux.onehotbatch&#40;truth, &#91;true, false&#93;&#41;
loader &#61; Flux.DataLoader&#40;&#40;noisy, target&#41;, batchsize&#61;64, shuffle&#61;true&#41;
for epoch in 1:1_000
    for xy_cpu in loader
        # Upload the batch to the GPU
        x, y &#61; xy_cpu |&gt; device
        loss, grads &#61; Flux.withgradient&#40;model&#41; do m
            y_hat &#61; m&#40;x&#41;
            Flux.logitcrossentropy&#40;y_hat, y&#41;
        end
        Flux.update&#33;&#40;opt_state, model, grads&#91;1&#93;&#41;
    end
end</code></pre>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->

    
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"></script>

<!-- Languages -->
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/julia.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/julia-repl.min.js"></script>

<script>hljs.highlightAll();</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="https://cdn.jsdelivr.net/npm/feather-icons@4.29.2/dist/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- BOOTSTRAP -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/js/bootstrap.bundle.min.js"></script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
