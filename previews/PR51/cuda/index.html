<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR51/assets/favicon.ico">
  <link rel="alternate" type="application/rss+xml" href="/previews/PR51/post/index.xml" title="RSS Feed for JuliaGPU">

  <link rel="stylesheet" href="/previews/PR51/css/bootstrap.min.css">
  
   <link rel="stylesheet" href="/previews/PR51/libs/highlight/github.min.css">
 

  <style>
 .hljs {
     padding: 0;
     background: 0 0
 }
.container {
   max-width: 700px
}

#nav-border {
   border-bottom: 1px solid #212529
}

#main {
   margin-top: 1em;
   margin-bottom: 4em
}

#home-jumbotron {
   background-color: inherit
}

#footer .container {
   padding: 1em 0
}

#footer a {
   color: inherit;
   text-decoration: underline
}

.font-125 {
   font-size: 125%
}

.tag-btn {
   margin-bottom: .3em
}

pre {
   background-color: #f5f5f5;
   border: 1px solid #ccc;
   border-radius: 4px;
   padding: 16px
}

pre code {
   padding: 0;
   font-size: inherit;
   color: inherit;
   background-color: transparent;
   border-radius: 0
}

code {
   padding: 2px 4px;
   font-size: 90%;
   color: #c7254e;
   background-color: #f9f2f4;
   border-radius: 4px
}

img,
iframe,
embed,
video,
audio {
   max-width: 100%
}

.card-img,
.card-img-top,
.card-img-bottom {
   width: initial
}

#main h1>a, #main h2>a, #main h3>a {
   color: inherit;
   text-decoration: none;
}

li p {
   margin: 0
}

</style>


  
  
    <title>NVIDIA CUDA ⋅ JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
        <ul class=navbar-nav>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/"><i data-feather=home></i>Home</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/post/"><i data-feather=file-text></i>Blog</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/learn/"><i data-feather=book-open></i>Learn</a>
            </li>
            <li class="nav-item active"><a class=nav-link href="/previews/PR51/cuda/">CUDA</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/rocm/">ROCm</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/oneapi/">oneAPI</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/metal/">Metal</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR51/other/">Other</a>
            </li>
        </ul>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
<!-- Content appended here -->

<h1 id="nvidia_cuda"><a href="#nvidia_cuda" class="header-anchor">NVIDIA CUDA</a></h1>

<p>
<a href="https://cuda.juliagpu.org/stable/">
  <img src="https://img.shields.io/badge/docs-stable-blue.svg" alt>
</a>
<a href="https://github.com/JuliaGPU/CUDA.jl">
  <img src="https://img.shields.io/github/stars/JuliaGPU/CUDA.jl?style=social" alt>
</a>
</p>

<p>The programming support for NVIDIA GPUs in Julia is provided by the <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> package. It is built on the CUDA toolkit, and aims to be as full-featured and offer the same performance as CUDA C. The toolchain is mature, has been under development since 2014 and can easily be installed on any current version of Julia using the integrated package manager.</p>
<p>CUDA.jl makes it possible to program NVIDIA GPUs at different abstraction levels:</p>
<ul>
<li><p>by using the <code>CuArray</code> type, providing a user-friendly yet powerful abstraction that does not require any GPU programming experience;</p>
</li>
<li><p>by writing CUDA kernels, with the same performance as kernels written in CUDA C;</p>
</li>
<li><p>by interfacing with CUDA APIs and libraries directly, offering the same level of flexibility you would expect from a C-based programming environment.</p>
</li>
</ul>
<p>The <a href="https://cuda.juliagpu.org/stable/">documentation</a> of CUDA.jl demonstrates each of these approaches.</p>
<h2 id="getting_started"><a href="#getting_started" class="header-anchor">Getting started</a></h2>
<p>CUDA.jl can be easily installed through Julia&#39;s package manager. You only need to install the NVIDIA driver; CUDA.jl will automatically download and install a compatible CUDA toolkit:</p>
<pre><code class="language-julia">pkg&gt; add CUDA</code></pre>
<p>Once you have the package installed, you can import it and start using it, e.g., via the <code>CuArray</code> array abstraction:</p>
<pre><code class="language-julia">julia&gt; using CUDA

julia&gt; A &#61; CuArray&#40;ones&#40;5, 5&#41;&#41;;

julia&gt; A .&#43; 1
5×5 CuMatrix&#123;Float64, CUDA.DeviceMemory&#125;:
 2.0  2.0  2.0  2.0  2.0
 2.0  2.0  2.0  2.0  2.0
 2.0  2.0  2.0  2.0  2.0
 2.0  2.0  2.0  2.0  2.0
 2.0  2.0  2.0  2.0  2.0</code></pre>
<p>Array operations like the <code>A .&#43; 1</code> above are automatically offloaded to the GPU, and allow for a very high-level programming style. If you need more control, you can write your own CUDA kernels directly in Julia:</p>
<pre><code class="language-julia">julia&gt; function add_one&#40;A&#41;
           i &#61; threadIdx&#40;&#41;.x
           A&#91;i&#93; &#43;&#61; 1
           return
       end

julia&gt; @cuda threads&#61;length&#40;A&#41; add_one&#40;A&#41;;

julia&gt; A
5×5 CuMatrix&#123;Float64, CUDA.DeviceMemory&#125;:
 3.0  3.0  3.0  3.0  3.0
 3.0  3.0  3.0  3.0  3.0
 3.0  3.0  3.0  3.0  3.0
 3.0  3.0  3.0  3.0  3.0
 3.0  3.0  3.0  3.0  3.0</code></pre>
<p>Even though in the example above CUDA.jl takes care of low-level details like memory management or device synchronization, it is still possible to do this yourself by directly calling into the underlying CUDA APIs. For more details, refer to the <a href="https://cuda.juliagpu.org/stable/lib/driver/">CUDA.jl documentation</a>.</p>
<h2 id="support"><a href="#support" class="header-anchor">Support</a></h2>
<p>CUDA.jl aims to support:</p>
<ul>
<li><p>all current versions of Julia, starting from the latest LTS;</p>
</li>
<li><p>the current and previous major version of the CUDA toolkit;</p>
</li>
<li><p>all platforms that the CUDA toolkit supports &#40;i.e., x86 and arm64, Linux and Window&#41;, including support for embedded devices like NVIDIA Jetson.</p>
</li>
</ul>
<p>In terms of API coverage, CUDA.jl aims to cover all APIs provided by the CUDA toolkkit, including its libraries like cuBLAS, cuFFT, cuSPARSE, etc. Other common libraries, like cuDNN, are also supported. For each of these, we always provide low-level bindings to the CUDA APIs &#40;making it possible to easily port existing code&#41;, while also providing high-level wrappers for most common APIs.</p>
<h2 id="performance"><a href="#performance" class="header-anchor">Performance</a></h2>
<p>Julia on the CPU is known for its good performance, approaching that of statically compiled languages like C. The same holds for programming NVIDIA GPUs with kernels written using CUDA.jl, where we have <a href="https://www.sciencedirect.com/science/article/pii/S0965997818310123">shown</a> the performance to approach and even sometimes exceed that of CUDA C on a selection<sup id="fnref:1"><a href="#fndef:1" class="fnref">[1]</a></sup> of applications from the Rodinia benchmark suite:</p>

<div class="card mb-3">
  <a href="/previews/PR51/assets/img/cuda-performance.png">
    <img src="/previews/PR51/assets/img/cuda-performance.png" class=card-img-top alt>
  </a>
  <div class=card-body>
    <p class=card-text>
      Relative performance of Rodinia benchmarks <a href=https://github.com/JuliaParallel/rodinia>implemented in Julia with CUDA.jl</a>.
    </p>
  </div>
</div>

<hr />
<p><table class="fndef" id="fndef:1">
    <tr>
        <td class="fndef-backref"><a href="#fnref:1">[1]</a></td>
        <td class="fndef-content">Since porting applications from one programming language to another is labour</td>
    </tr>
</table>
intensive, we only ported and analyzed the 10 smallest benchmarks from the suite. More details can be found in <a href="https://www.sciencedirect.com/science/article/pii/S0965997818310123">the paper</a>.</p>

<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->


    
    
        <script src="/previews/PR51/libs/highlight/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="/previews/PR51/libs/feather/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
