<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR18/assets/favicon.ico">
  <link rel=alternate type=application/rss+xml href=https://juliagpu.org/index.xml title=JuliaGPU>

  <link rel=stylesheet href="/previews/PR18/css/bootstrap.min.css">
  
   <link rel="stylesheet" href="/previews/PR18/libs/highlight/github.min.css">
 

  <style>
 .hljs {
     padding: 0;
     background: 0 0
 }
.container {
   max-width: 700px
}

#nav-border {
   border-bottom: 1px solid #212529
}

#main {
   margin-top: 1em;
   margin-bottom: 4em
}

#home-jumbotron {
   background-color: inherit
}

#footer .container {
   padding: 1em 0
}

#footer a {
   color: inherit;
   text-decoration: underline
}

.font-125 {
   font-size: 125%
}

.tag-btn {
   margin-bottom: .3em
}

pre {
   background-color: #f5f5f5;
   border: 1px solid #ccc;
   border-radius: 4px;
   padding: 16px
}

pre code {
   padding: 0;
   font-size: inherit;
   color: inherit;
   background-color: transparent;
   border-radius: 0
}

code {
   padding: 2px 4px;
   font-size: 90%;
   color: #c7254e;
   background-color: #f9f2f4;
   border-radius: 4px
}

img,
iframe,
embed,
video,
audio {
   max-width: 100%
}

.card-img,
.card-img-top,
.card-img-bottom {
   width: initial
}

#main h1>a, #main h2>a, #main h3>a {
   color: inherit;
   text-decoration: none;
}

li p {
   margin: 0
}

</style>


  
  
    <title>CUDA.jl 3.4 ⋅ JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
        <ul class=navbar-nav>
            <li class="nav-item "><a class=nav-link href="/previews/PR18/"><i data-feather=home></i>Home</a>
            </li>
            <li class="nav-item active"><a class=nav-link href="/previews/PR18/post/"><i data-feather=file-text></i>Blog</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR18/learn/"><i data-feather=book-open></i>Learn</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR18/cuda/">NVIDIA CUDA</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR18/rocm/">AMD ROCm</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR18/oneapi/">Intel oneAPI</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR18/other/">Other</a>
            </li>
        </ul>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
      <!-- make sure to generate a Hugo-era URI at the root, redirecting to /post/.... -->
      

      <h1>CUDA.jl 3.4</h1>
      <i data-feather=calendar></i>
<time datetime=2021-8-13>Aug 13, 2021</time><br>
<i data-feather=edit-2></i>
Tim Besard


      <br><br>
    
<!-- Content appended here -->

<p>The latest version of CUDA.jl brings several new features, from improved atomic operations to initial support for arrays with unified memory. The native random number generator introduced in CUDA.jl 3.0 is now the default fallback, and support for memory pools other than the CUDA stream-ordered one has been removed.</p>
<h2 id="streamlined_atomic_operations"><a href="#streamlined_atomic_operations" class="header-anchor">Streamlined atomic operations</a></h2>
<p>In preparation of integrating with the new standard <code>@atomic</code> macro introduced in Julia 1.7, we have <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1059">streamlined the capabilities of atomic operations in CUDA.jl</a>. The API is now split into two levels: low-level <code>atomic_</code> methods for atomic functionality that&#39;s directly supported by the hardware, and a high-level <code>@atomic</code> macro that tries to perform operations natively or falls back to a loop with compare-and-swap. This fall-back implementation makes it possible to use more complex operations that do not map onto a single atomic operation:</p>
<pre><code class="language-julia-repl">julia&gt; a &#61; CuArray&#40;&#91;1&#93;&#41;;

julia&gt; function kernel&#40;a&#41;
         CUDA.@atomic a&#91;&#93; &lt;&lt;&#61; 1
         return
       end

julia&gt; @cuda threads&#61;16 kernel&#40;a&#41;

julia&gt; a
1-element CuArray&#123;Int64, 1, CUDA.Mem.DeviceBuffer&#125;:
 65536

julia&gt; 1&lt;&lt;16
65536</code></pre>
<p>The only requirement is that the types being used are supported by <code>CUDA.atomic_cas&#33;</code>. This includes common types like 32 and 64-bit integers and floating-point numbers, as well as 16-bit numbers on devices with compute capability 7.0 or higher.</p>
<p>Note that on Julia 1.7 and higher, CUDA.jl <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1097">does not export the <code>@atomic</code> macro anymore</a> to avoid conflicts with the version in Base. That means it is recommended to always fully specify uses of the macro, i.e., use <code>CUDA.@atomic</code> as in the example above.</p>
<h2 id="arrays_with_unified_memory"><a href="#arrays_with_unified_memory" class="header-anchor">Arrays with unified memory</a></h2>
<p>You may have noticed that the <code>CuArray</code> type in the example above included an additional parameter, <code>Mem.DeviceBuffer</code>. This has been introduced to support arrays backed by different kinds of buffers. By default, we will use an ordinary device buffer, but it&#39;s now possible to <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1023">allocate arrays backed by unified buffers</a> that can be used on multiple devices:</p>
<pre><code class="language-julia-repl">julia&gt; a &#61; cu&#40;&#91;0&#93;; unified&#61;true&#41;
1-element CuArray&#123;Int64, 1, CUDA.Mem.UnifiedBuffer&#125;:
 0

julia&gt; a .&#43;&#61; 1
1-element CuArray&#123;Int64, 1, CUDA.Mem.UnifiedBuffer&#125;:
 1

julia&gt; device&#33;&#40;1&#41;

julia&gt; a .&#43;&#61; 1
1-element CuArray&#123;Int64, 1, CUDA.Mem.UnifiedBuffer&#125;:
 2</code></pre>
<p>Although all operations should work equally well with arrays backed by unified memory, they have not been optimized yet. For example, copying memory to the device could be avoided as the driver can automatically page in unified memory on-demand.</p>
<h2 id="new_default_random_number_generator"><a href="#new_default_random_number_generator" class="header-anchor">New default random number generator</a></h2>
<p>CUDA.jl 3.0 introduced a new random number generator, and starting with CUDA.jl 3.2 performance and quality of this generator was improved up to the point it could be used by applications. A couple of features were still missing though, such as generating normally-distributed random numbers, or support for complex numbers. These features have been <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1082">added in CUDA.jl 3.3</a>, and the generator is now used as the default fallback when CURAND does not support the requested element types.</p>
<p>Both the performance and quality of this generator is much better than the previous, GPUArrays.jl-based one:</p>
<pre><code class="language-julia-repl">julia&gt; using BenchmarkTools
julia&gt; cuda_rng &#61; CUDA.RNG&#40;&#41;;
julia&gt; gpuarrays_rng &#61; GPUArrays.default_rng&#40;CuArray&#41;;
julia&gt; a &#61; CUDA.zeros&#40;1024,1024&#41;;

julia&gt; @benchmark CUDA.@sync rand&#33;&#40;&#36;cuda_rng, &#36;a&#41;
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range &#40;min … max&#41;:  17.040 μs …  2.430 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 99.04&#37;
 Time  &#40;median&#41;:     18.500 μs              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   20.604 μs ± 34.734 μs  ┊ GC &#40;mean ± σ&#41;:  1.17&#37; ±  0.99&#37;

         ▃▆█▇▇▅▄▂▁
  ▂▂▂▃▄▆███████████▇▆▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂ ▄
  17 μs           Histogram: frequency by time        24.1 μs &lt;

julia&gt; @benchmark CUDA.@sync rand&#33;&#40;&#36;gpuarrays_rng, &#36;a&#41;
BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range &#40;min … max&#41;:  72.489 μs …  2.790 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 98.44&#37;
 Time  &#40;median&#41;:     74.479 μs              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   81.211 μs ± 61.598 μs  ┊ GC &#40;mean ± σ&#41;:  0.67&#37; ±  1.40&#37;

  █                                                           ▁
  █▆▃▁▃▃▅▆▅▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▄▆▁▁▁▁▁▁▁▁▄▄▃▄▃▁▁▁▁▁▁▁▁▁▃▃▄▆▄▁▄▃▆ █
  72.5 μs      Histogram: log&#40;frequency&#41; by time       443 μs &lt;</code></pre>
<pre><code class="language-julia-repl">julia&gt; using RNGTest
julia&gt; test_cuda_rng &#61; RNGTest.wrap&#40;cuda_rng, UInt32&#41;;
julia&gt; test_gpuarrays_rng &#61; RNGTest.wrap&#40;gpuarrays_rng, UInt32&#41;;

julia&gt; RNGTest.smallcrushTestU01&#40;test_cuda_rng&#41;
 All tests were passed

julia&gt; RNGTest.smallcrushTestU01&#40;test_gpuarrays_rng&#41;
 The following tests gave p-values outside &#91;0.001, 0.9990&#93;:

       Test                          p-value
 ----------------------------------------------
  1  BirthdaySpacings                 eps
  2  Collision                        eps
  3  Gap                              eps
  4  SimpPoker                       1.0e-4
  5  CouponCollector                  eps
  6  MaxOft                           eps
  7  WeightDistrib                    eps
 10  RandomWalk1 M                   6.0e-4
 ----------------------------------------------
 &#40;eps  means a value &lt; 1.0e-300&#41;:</code></pre>
<h2 id="removal_of_old_memory_pools"><a href="#removal_of_old_memory_pools" class="header-anchor">Removal of old memory pools</a></h2>
<p>With the new stream-ordered allocator, caching memory allocations at the CUDA library level, much of the need for memory pools to cache memory allocations has disappeared. To simplify the allocation code, we have <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1015">removed support for those Julia-managed memory pools</a> &#40;i.e., <code>binned</code>, <code>split</code> and <code>simple</code>&#41;. You can now only use the <code>cuda</code> memory pool, or use no pool at all by setting the <code>JULIA_CUDA_MEMORY_POOL</code> environment variable to <code>none</code>.</p>
<p>Not using a memory pool degrades performance, so if you are stuck on an NVIDIA driver that does not support CUDA 11.2, it is advised to remain on CUDA.jl 3.3 until you can upgrade.</p>
<p>Also note that the new stream-ordered allocator has <a href="https://github.com/JuliaGPU/CUDA.jl/issues/1053">turned out incompatible with legacy cuIpc APIs</a> as used by OpenMPI. If that applies to you, consider disabling the memory pool or reverting to CUDA.jl 3.3 if your application&#39;s allocation pattern benefits from a memory pool.</p>
<p>Because of this, we will be maintaining CUDA.jl 3.3 longer than usual. All bug fixes in CUDA.jl 3.4 have already been backported to the previous release, which is currently at version 3.3.6.</p>
<h2 id="device_capability-dependent_kernel_code"><a href="#device_capability-dependent_kernel_code" class="header-anchor">Device capability-dependent kernel code</a></h2>
<p>Some of the improvements in this release depend on the ability to write generic code that only uses certain hardware features when they are available. To facilitate writing such code, the compiler now embeds metadata in the generated code that can be used to branch on.</p>
<p>Currently, the device capability and PTX ISA version are embedded and made available using respectively the <code>compute_capability</code> and <code>ptx_isa_version</code> functions. A simplified version number type, constructable using the <code>sv&quot;...&quot;</code> string macro, can be used to test against these properties. For example:</p>
<pre><code class="language-julia-repl">julia&gt; function kernel&#40;a&#41;
           a&#91;&#93; &#61; compute_capability&#40;&#41; &gt;&#61; sv&quot;6.0&quot; ? 1 : 2
           return
       end
kernel &#40;generic function with 1 method&#41;

julia&gt; CUDA.code_llvm&#40;kernel, Tuple&#123;CuDeviceVector&#123;Float32, AS.Global&#125;&#125;&#41;
define void @julia_kernel_1&#40;&#123; i8 addrspace&#40;1&#41;*, i64, &#91;1 x i64&#93; &#125;* &#37;0&#41; &#123;
top:
  &#37;1 &#61; bitcast &#123; i8 addrspace&#40;1&#41;*, i64, &#91;1 x i64&#93; &#125;* &#37;0 to float addrspace&#40;1&#41;**
  &#37;2 &#61; load float addrspace&#40;1&#41;*, float addrspace&#40;1&#41;** &#37;1, align 8
  store float 1.000000e&#43;00, float addrspace&#40;1&#41;* &#37;2, align 4
  ret void
&#125;

julia&gt; capability&#40;device&#33;&#40;1&#41;&#41;
v&quot;3.5.0&quot;

julia&gt; CUDA.code_llvm&#40;kernel, Tuple&#123;CuDeviceVector&#123;Float32, AS.Global&#125;&#125;&#41;
define void @julia_kernel_2&#40;&#123; i8 addrspace&#40;1&#41;*, i64, &#91;1 x i64&#93; &#125;* &#37;0&#41; &#123;
top:
  &#37;1 &#61; bitcast &#123; i8 addrspace&#40;1&#41;*, i64, &#91;1 x i64&#93; &#125;* &#37;0 to float addrspace&#40;1&#41;**
  &#37;2 &#61; load float addrspace&#40;1&#41;*, float addrspace&#40;1&#41;** &#37;1, align 8
  store float 2.000000e&#43;00, float addrspace&#40;1&#41;* &#37;2, align 4
  ret void
&#125;</code></pre>
<p>The branch on the compute capability is completely optimized away. At the same time, this does not require re-inferring the function as the optimization happens at the LLVM level.</p>
<h2 id="other_changes"><a href="#other_changes" class="header-anchor">Other changes</a></h2>
<ul>
<li><p><a href="https://github.com/JuliaGPU/CUDA.jl/pull/1084">Support for CUDA 11.4 Update 1</a></p>
</li>
<li><p>Improved thread safety <a href="https://github.com/JuliaGPU/CUDA.jl/pull/993">&#91;1&#93;</a> <a href="https://github.com/JuliaGPU/CUDA.jl/pull/1074">&#91;2&#93;</a></p>
</li>
</ul>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->


    
    
        <script src="/previews/PR18/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="/previews/PR18/libs/feather/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
