<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR39/assets/favicon.ico">
  <link rel="alternate" type="application/rss+xml" href="/previews/PR39/post/index.xml" title="RSS Feed for JuliaGPU">

  <link rel="stylesheet" href="/previews/PR39/css/bootstrap.min.css">
  
   <link rel="stylesheet" href="/previews/PR39/libs/highlight/github.min.css">
 

  <style>
 .hljs {
     padding: 0;
     background: 0 0
 }
.container {
   max-width: 700px
}

#nav-border {
   border-bottom: 1px solid #212529
}

#main {
   margin-top: 1em;
   margin-bottom: 4em
}

#home-jumbotron {
   background-color: inherit
}

#footer .container {
   padding: 1em 0
}

#footer a {
   color: inherit;
   text-decoration: underline
}

.font-125 {
   font-size: 125%
}

.tag-btn {
   margin-bottom: .3em
}

pre {
   background-color: #f5f5f5;
   border: 1px solid #ccc;
   border-radius: 4px;
   padding: 16px
}

pre code {
   padding: 0;
   font-size: inherit;
   color: inherit;
   background-color: transparent;
   border-radius: 0
}

code {
   padding: 2px 4px;
   font-size: 90%;
   color: #c7254e;
   background-color: #f9f2f4;
   border-radius: 4px
}

img,
iframe,
embed,
video,
audio {
   max-width: 100%
}

.card-img,
.card-img-top,
.card-img-bottom {
   width: initial
}

#main h1>a, #main h2>a, #main h3>a {
   color: inherit;
   text-decoration: none;
}

li p {
   margin: 0
}

</style>


  
  
    <title>oneAPI.jl 1.0: oneMKL, Intel Arc and Julia 1.9 ⋅ JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
        <ul class=navbar-nav>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/"><i data-feather=home></i>Home</a>
            </li>
            <li class="nav-item active"><a class=nav-link href="/previews/PR39/post/"><i data-feather=file-text></i>Blog</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/learn/"><i data-feather=book-open></i>Learn</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/cuda/">CUDA</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/rocm/">ROCm</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/oneapi/">oneAPI</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/metal/">Metal</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR39/other/">Other</a>
            </li>
        </ul>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
      <!-- make sure to generate a Hugo-era URI at the root, redirecting to /post/.... -->
      

      <h1>oneAPI.jl 1.0: oneMKL, Intel Arc and Julia 1.9</h1>
      <i data-feather=calendar></i>
<time datetime=2023-2-8>Feb 8, 2023</time><br>
<i data-feather=edit-2></i>
Tim Besard


      <br><br>
    
<!-- Content appended here -->

<p>The release of oneAPI.jl 1.0 adds integration with the oneAPI Math Kernel Library &#40;oneMKL&#41; to accelerate linear algebra operations on Intel GPUs. It also brings support for Julia 1.9 and Intel Arc GPUs.</p>
<h2 id="onemkl_integration"><a href="#onemkl_integration" class="header-anchor">oneMKL integration</a></h2>
<p>oneAPI.jl now uses the Intel oneAPI Math Kernel Library &#40;oneMKL&#41;, automatically downloaded as part of <code>oneAPI_Support_jll.jl</code>, to accelerate a great number of BLAS and LAPACK operations on Intel GPUs. Similar to how it is implemented in our other GPU back-ends, these wrappers are available at different levels of abstraction.</p>
<p>At the lowest level, we use a C library that wraps the oneMKL C&#43;&#43; APIs. For example, the <code>oneapi::mkl::blas::column_major::gemm</code> function for matrix-matrix multiplication is wrapped by the C functions <code>onemklSgemm</code>, <code>onemklDgemm</code>, etc. These wrappers are used to implement low-level methods like <code>oneMKL.gemm&#33;</code>:</p>
<pre><code class="language-julia-repl">julia&gt; using oneAPI

julia&gt; A &#61; oneArray&#40;rand&#40;Float32, 2, 3&#41;&#41;;
2×3 oneMatrix&#123;Float32, oneAPI.oneL0.DeviceBuffer&#125;:
 0.44302   0.125576  0.859145
 0.674291  0.428346  0.0400119
julia&gt; B &#61; oneArray&#40;rand&#40;Float32, 3, 4&#41;&#41;
3×4 oneMatrix&#123;Float32, oneAPI.oneL0.DeviceBuffer&#125;:
 0.592748   0.529413   0.0323396  0.659528
 0.22489    0.0872259  0.253291   0.376519
 0.0121506  0.591135   0.706755   0.751686
julia&gt; C &#61; similar&#40;B, &#40;2, 4&#41;&#41;;

julia&gt; oneMKL.gemm&#33;&#40;&#39;N&#39;, &#39;N&#39;, true, A, B, true, C&#41;
2×4 oneMatrix&#123;Float32, oneAPI.oneL0.DeviceBuffer&#125;:
 0.301279  0.753365  0.65334   0.985274
 0.496501  0.417994  0.158581  0.63607

julia&gt; Array&#40;C&#41; ≈ Array&#40;A&#41; * Array&#40;B&#41;
true</code></pre>
<p>Of course, these low-level functions aren&#39;t very user-friendly, so we also integrate with Julia&#39;s standard libraries where possible:</p>
<pre><code class="language-julia-repl">julia&gt; A &#61; oneArray&#40;rand&#40;Float32, 2, 3&#41;&#41;;
julia&gt; B &#61; oneArray&#40;rand&#40;Float32, 3, 4&#41;&#41;;

julia&gt; using LinearAlgebra
julia&gt; C &#61; A * B;

julia&gt; Array&#40;C&#41; ≈ Array&#40;A&#41; * Array&#40;B&#41;
true</code></pre>
<p>The most frequently used oneMKL BLAS functions have been wrapped and integrated with Julia’s standard linear algebra libraries. If you run into a missing function, please file a request to add it, or take a look at the source and contribute to oneAPI.jl&#33; The current state of the wrappers should make it easy to extend their functionality, as well as form a good basis for integrating with other libraries like oneDNN.</p>
<h2 id="intel_arc_support"><a href="#intel_arc_support" class="header-anchor">Intel Arc support</a></h2>
<p>The new Arc series of discrete Intel GPUs are now fully supported by oneAPI.jl. These GPUs offer a significant performance improvement over their integrated predecessors:</p>
<pre><code class="language-julia-repl">julia&gt; using oneAPI
julia&gt; oneAPI.versioninfo&#40;&#41;
1 device:
- Intel&#40;R&#41; Arc&#40;TM&#41; A770 Graphics &#91;0x56a0&#93;

julia&gt; T &#61; Float32;
julia&gt; n &#61; p &#61; m &#61; 2048;
julia&gt; a &#61; oneArray&#40;rand&#40;T, n, p&#41;&#41;;
julia&gt; b &#61; oneArray&#40;rand&#40;T, p, m&#41;&#41;;
julia&gt; c &#61; oneArray&#40;zeros&#40;T, n, m&#41;&#41;;

julia&gt; using BenchmarkTools, LinearAlgebra
julia&gt; bench &#61; @benchmark oneAPI.@sync mul&#33;&#40;c, a, b&#41;
BenchmarkTools.Trial: 1510 samples with 1 evaluation.
 Range &#40;min … max&#41;:  3.233 ms …  3.791 ms  ┊ GC &#40;min … max&#41;: 0.00&#37; … 0.00&#37;
 Time  &#40;median&#41;:     3.298 ms              ┊ GC &#40;median&#41;:    0.00&#37;
 Time  &#40;mean ± σ&#41;:   3.308 ms ± 48.426 μs  ┊ GC &#40;mean ± σ&#41;:  0.00&#37; ± 0.00&#37;

        ▁▃▄▇█▅▄▃▂   ▁▁▁
  ▁▁▃▃▅▇██████████████████▇▇▇▅▆▄▅▅▄▂▃▂▂▂▂▂▂▁▂▂▂▁▂▁▂▁▂▂▂▂▁▁▂▂ ▃
  3.23 ms        Histogram: frequency by time        3.47 ms &lt;

 Memory estimate: 272 bytes, allocs estimate: 11.

julia&gt; flops &#61; n*m*&#40;2p-1&#41;
17175674880

julia&gt; flops / &#40;minimum&#40;bench.times&#41;/1e9&#41;
5.3131281169900205e12</code></pre>
<p>For example, here we&#39;re getting over 5 TFlops of Float32 performance, which is over 10x faster than the Intel Xe Graphics G7 we had been previously using for oneAPI.jl development. At the same time, the A770 used above should be able to deliver close to 20 TFlops, so there&#39;s still room for improvement in our software stack.</p>
<p>To use oneAPI.jl with an Arc series GPU, you need to run Linux 6.2. At the time of writing, that kernel is still in beta, so refer to your distribution&#39;s documentation for how to install it. For example, on Arch Linux you can use the <a href="https://aur.archlinux.org/packages/linux-mainline"><code>linux-mainline</code> package from the AUR</a>, Ubuntu has the <a href="https://wiki.ubuntu.com/Kernel/MainlineBuilds"><code>kernel-ppa</code> archive</a>, Fedora provides the <a href="https://fedoraproject.org/wiki/Kernel_Vanilla_Repositories"><code>stable-rc</code> repository</a>, etc.</p>
<h2 id="other_changes"><a href="#other_changes" class="header-anchor">Other changes</a></h2>
<ul>
<li><p>Support for Julia 1.9 has been added.</p>
</li>
</ul>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->


    
    
        <script src="/previews/PR39/libs/highlight/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="/previews/PR39/libs/feather/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
