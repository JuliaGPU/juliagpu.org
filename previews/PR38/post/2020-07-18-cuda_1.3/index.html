<!doctype html>
<html lang="en" class=h-100>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
  <meta content="index, follow" name=robots>
  <link rel="icon" href="/previews/PR38/assets/favicon.ico">
  <link rel="alternate" type="application/rss+xml" href="/previews/PR38/post/index.xml" title="RSS Feed for JuliaGPU">

  <link rel="stylesheet" href="/previews/PR38/css/bootstrap.min.css">
  
   <link rel="stylesheet" href="/previews/PR38/libs/highlight/github.min.css">
 

  <style>
 .hljs {
     padding: 0;
     background: 0 0
 }
.container {
   max-width: 700px
}

#nav-border {
   border-bottom: 1px solid #212529
}

#main {
   margin-top: 1em;
   margin-bottom: 4em
}

#home-jumbotron {
   background-color: inherit
}

#footer .container {
   padding: 1em 0
}

#footer a {
   color: inherit;
   text-decoration: underline
}

.font-125 {
   font-size: 125%
}

.tag-btn {
   margin-bottom: .3em
}

pre {
   background-color: #f5f5f5;
   border: 1px solid #ccc;
   border-radius: 4px;
   padding: 16px
}

pre code {
   padding: 0;
   font-size: inherit;
   color: inherit;
   background-color: transparent;
   border-radius: 0
}

code {
   padding: 2px 4px;
   font-size: 90%;
   color: #c7254e;
   background-color: #f9f2f4;
   border-radius: 4px
}

img,
iframe,
embed,
video,
audio {
   max-width: 100%
}

.card-img,
.card-img-top,
.card-img-bottom {
   width: initial
}

#main h1>a, #main h2>a, #main h3>a {
   color: inherit;
   text-decoration: none;
}

li p {
   margin: 0
}

</style>


  
  
    <title>CUDA.jl 1.3 - Multi-device programming â‹… JuliaGPU</title>
  
</head>
<body class="d-flex flex-column h-100">
  <div id=nav-border class=container>
    <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
        <ul class=navbar-nav>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/"><i data-feather=home></i>Home</a>
            </li>
            <li class="nav-item active"><a class=nav-link href="/previews/PR38/post/"><i data-feather=file-text></i>Blog</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/learn/"><i data-feather=book-open></i>Learn</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/cuda/">CUDA</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/rocm/">ROCm</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/oneapi/">oneAPI</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/metal/">Metal</a>
            </li>
            <li class="nav-item "><a class=nav-link href="/previews/PR38/other/">Other</a>
            </li>
        </ul>
    </nav>
</div>


  <div class="container">
    <main id=main>

    
      <!-- make sure to generate a Hugo-era URI at the root, redirecting to /post/.... -->
      

      <h1>CUDA.jl 1.3 - Multi-device programming</h1>
      <i data-feather=calendar></i>
<time datetime=2020-7-18>Jul 18, 2020</time><br>
<i data-feather=edit-2></i>
Tim Besard


      <br><br>
    
<!-- Content appended here -->

<p>Today we&#39;re releasing CUDA.jl 1.3, with several new features. The most prominent change is support for multiple GPUs within a single process.</p>
<h2 id="multi-gpu_programming"><a href="#multi-gpu_programming" class="header-anchor">Multi-GPU programming</a></h2>
<p>With CUDA.jl 1.3, you can finally use multiple CUDA GPUs within a single process. To switch devices you can call <code>device&#33;</code>, query the current device with <code>device&#40;&#41;</code>, or reset it using <code>device_reset&#33;&#40;&#41;</code>:</p>
<pre><code class="language-julia-repl">julia&gt; collect&#40;devices&#40;&#41;&#41;
9-element Array&#123;CuDevice,1&#125;:
 CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB
 CuDevice&#40;1&#41;: Tesla V100-PCIE-32GB
 CuDevice&#40;2&#41;: Tesla V100-PCIE-32GB
 CuDevice&#40;3&#41;: Tesla V100-PCIE-32GB
 CuDevice&#40;4&#41;: Tesla V100-PCIE-16GB
 CuDevice&#40;5&#41;: Tesla P100-PCIE-16GB
 CuDevice&#40;6&#41;: Tesla P100-PCIE-16GB
 CuDevice&#40;7&#41;: GeForce GTX 1080 Ti
 CuDevice&#40;8&#41;: GeForce GTX 1080 Ti

julia&gt; device&#33;&#40;5&#41;

julia&gt; device&#40;&#41;
CuDevice&#40;5&#41;: Tesla P100-PCIE-16GB</code></pre>
<p>Let&#39;s define a kernel to show this really works:</p>
<pre><code class="language-julia-repl">julia&gt; function kernel&#40;&#41;
           dev &#61; Ref&#123;Cint&#125;&#40;&#41;
           CUDA.cudaGetDevice&#40;dev&#41;
           @cuprintln&#40;&quot;Running on device &#36;&#40;dev&#91;&#93;&#41;&quot;&#41;
           return
       end

julia&gt; @cuda kernel&#40;&#41;
Running on device 5

julia&gt; device&#33;&#40;0&#41;

julia&gt; device&#40;&#41;
CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB

julia&gt; @cuda kernel&#40;&#41;
Running on device 0</code></pre>
<p>Memory allocations, like <code>CuArray</code>s, are implicitly bound to the device they were allocated on. That means you should take care to only use an array when the owning device is active, or you will run into errors:</p>
<pre><code class="language-julia-repl">julia&gt; device&#40;&#41;
CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB

julia&gt; a &#61; CUDA.rand&#40;1&#41;
1-element CuArray&#123;Float32,1&#125;:
 0.6322775

julia&gt; device&#33;&#40;1&#41;

julia&gt; a
ERROR: CUDA error: an illegal memory access was encountered</code></pre>
<p>Future improvements might make the array type device-aware.</p>
<h2 id="multitasking_and_multithreading"><a href="#multitasking_and_multithreading" class="header-anchor">Multitasking and multithreading</a></h2>
<p>Dovetailing with the support for multiple GPUs, is the ability to use these GPUs on separate Julia tasks and threads:</p>
<pre><code class="language-julia-repl">julia&gt; device&#33;&#40;0&#41;

julia&gt; @sync begin
         @async begin
           device&#33;&#40;1&#41;
           println&#40;&quot;Working with &#36;&#40;device&#40;&#41;&#41; on &#36;&#40;current_task&#40;&#41;&#41;&quot;&#41;
           yield&#40;&#41;
           println&#40;&quot;Back to device &#36;&#40;device&#40;&#41;&#41; on &#36;&#40;current_task&#40;&#41;&#41;&quot;&#41;
         end
         @async begin
           device&#33;&#40;2&#41;
           println&#40;&quot;Working with &#36;&#40;device&#40;&#41;&#41; on &#36;&#40;current_task&#40;&#41;&#41;&quot;&#41;
         end
       end
Working with CuDevice&#40;1&#41; on Task @0x00007fc9e6a48010
Working with CuDevice&#40;2&#41; on Task @0x00007fc9e6a484f0
Back to device CuDevice&#40;1&#41; on Task @0x00007fc9e6a48010

julia&gt; device&#40;&#41;
CuDevice&#40;0&#41;: Tesla V100-PCIE-32GB</code></pre>
<p>Each task has its own local GPU state, such as the device it was bound to, handles to libraries like CUBLAS or CUDNN &#40;which means that each task can configure libraries independently&#41;, etc.</p>
<h2 id="minor_features"><a href="#minor_features" class="header-anchor">Minor features</a></h2>
<p>CUDA.jl 1.3 also features some minor changes:</p>
<ul>
<li><p>Reinstated compatibility with Julia 1.3</p>
</li>
<li><p>Support for CUDA 11.0 Update 1</p>
</li>
<li><p>Support for CUDNN 8.0.2</p>
</li>
</ul>
<h2 id="known_issues"><a href="#known_issues" class="header-anchor">Known issues</a></h2>
<p>Several operations on sparse arrays have been broken since CUDA.jl 1.2, due to the deprecations that were part of CUDA 11. The next version of CUDA.jl will drop support for CUDA 10.0 or older, which will make it possible to use new cuSPARSE APIs and add back missing functionality.</p>
<!-- CONTENT ENDS HERE -->
      </main>
    </div> <!-- class="container" -->


    
    
        <script src="/previews/PR38/libs/highlight/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    

    <footer id=footer class="mt-auto text-center text-muted">
        <div class=container>Made with <a href=https://franklinjl.org>Franklin.jl</a> and <a href=https://julialang.org>the Julia programming language</a>.</div>
    </footer>

    <!-- FEATHER -->
    <script src="/previews/PR38/libs/feather/feather.min.js"></script>
    <script>feather.replace()</script>

    <!-- GOOGLE ANALYTICS -->
    <script>
    window.ga = window.ga || function() {
        (ga.q = ga.q || []).push(arguments)
    };
    ga.l = +new Date;
    ga('create', 'UA-154489943-1', 'auto');
    ga('send', 'pageview');
    </script>
    <script async src=https://www.google-analytics.com/analytics.js></script>

  </body>
</html>
