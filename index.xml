<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>JuliaGPU</title><link>https://juliagpu.org/</link><description>Recent content on JuliaGPU</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://juliagpu.org/index.xml" rel="self" type="application/rss+xml"/><item><title>CUDA.jl 2.4 and 2.5</title><link>https://juliagpu.org/2021-01-08-cuda_2.4_2.5/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2021-01-08-cuda_2.4_2.5/</guid><description>CUDA.jl v2.4 and v2.5 are two almost-identical feature releases, respectively for Julia 1.5 and 1.6. These releases feature a greatly improved findmin and findmax kernels, an improved interface for kernel introspection, support for CUDA 11.2, and of course many bug fixes.</description><content:encoded><![CDATA[<p>CUDA.jl v2.4 and v2.5 are two almost-identical feature releases, respectively for Julia 1.5
and 1.6. These releases feature a greatly improved <code>findmin</code> and <code>findmax</code> kernels, an
improved interface for kernel introspection, support for CUDA 11.2, and of course many bug
fixes.</p>
<h2 id="improved-findmin-and-findmax-kernels">Improved <code>findmin</code> and <code>findmax</code> kernels</h2>
<p>Thanks to <a href="https://github.com/tkf">@tkf</a> and <a href="https://github.com/Ellipse0934">@Ellipse0934</a>,
CUDA.jl now <a href="https://github.com/JuliaGPU/CUDA.jl/pull/576">uses a single-pass kernel for finding the minimum or maximum item in a
CuArray</a>. This fixes compatibility with
<code>NaN</code>-valued elements, while on average improving performance. Depending on the rank, shape
and size of the array these improvements vary from a minor regression to order-of-magnitude
improvements.</p>
<h2 id="new-kernel-introspection-interface">New kernel introspection interface</h2>
<p>It is now possible to obtain a compiled-but-not-launched kernel by passing the
<code>launch=false</code> keyword to <code>@cuda</code>. This is useful when you want to reflect, e.g., query the
amount of registers, or other kernel properties:</p>
<pre><code class="language-julia">julia&gt; kernel = @cuda launch=false identity(nothing)
CUDA.HostKernel{identity,Tuple{Nothing}}(...)

julia&gt; CUDA.registers(kernel)
4
</code></pre>
<p>The old API is still available, and will even be extended in future versions of CUDA.jl for
the purpose of compiling device functions (not kernels):</p>
<pre><code class="language-julia">julia&gt; kernel = cufunction(identity, Tuple{Nothing})
CUDA.HostKernel{identity,Tuple{Nothing}}(...)
</code></pre>
<h2 id="support-for-cuda-112">Support for CUDA 11.2</h2>
<p>CUDA.jl now supports the latest version of CUDA, version 11.2. Because CUDNN and CUTENSOR
are not compatible with this release yet, CUDA.jl won&rsquo;t automatically switch to it unless
you explicitly request so:</p>
<pre><code class="language-julia">julia&gt; ENV[&quot;JULIA_CUDA_VERSION&quot;] = &quot;11.2&quot;
&quot;11.2&quot;

julia&gt; using CUDA

julia&gt; CUDA.versioninfo()
CUDA toolkit 11.2.0, artifact installation
CUDA driver 11.2.0
NVIDIA driver 460.27.4
</code></pre>
<p>Alternatively, if you disable use of artifacts through <code>JULIA_CUDA_USE_BINARYBUILDER=false</code>,
CUDA 11.2 can be picked up from your local system.</p>
<h2 id="future-developments">Future developments</h2>
<p>Due to upstream compiler changes, CUDA.jl 2.4 is expected to be the last release compatible
with Julia 1.5. Patch releases are still possible, but are not automatic: If you need a
specific bugfix from a future CUDA.jl release, create an issue or PR to backport the change.</p>]]></content:encoded></item><item><title>Introducing: oneAPI.jl</title><link>https://juliagpu.org/2020-11-05-oneapi_0.1/</link><pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-11-05-oneapi_0.1/</guid><description>We&amp;rsquo;re proud to announce the first version of oneAPI.jl, a Julia package for programming accelerators with the oneAPI programming model. It is currently available for select Intel GPUs, including common integrated ones, and offers a similar experience to CUDA.jl.</description><content:encoded><![CDATA[<p>We&rsquo;re proud to announce the first version of oneAPI.jl, a Julia package for programming
accelerators with the <a href="https://www.oneapi.com/">oneAPI programming model</a>. It is currently
available for select Intel GPUs, including common integrated ones, and offers a similar
experience to CUDA.jl.</p>
<p>The initial version of this package, v0.1, consists of three key components:</p>
<ul>
<li>wrappers for the oneAPI Level Zero interfaces;</li>
<li>a compiler for Julia source code to SPIR-V IR;</li>
<li>and an array interface for convenient data-parallel programming.</li>
</ul>
<p>In this post, I&rsquo;ll briefly describe each of these. But first, some essentials.</p>
<h2 id="installation">Installation</h2>
<p>oneAPI.jl is currently only supported on 64-bit Linux, using a sufficiently recent kernel,
and requires Julia 1.5. Furthermore, it currently only supports a limited set of Intel GPUs:
Gen9 (Skylake, Kaby Lake, Coffee Lake), Gen11 (Ice Lake), and Gen12 (Tiger Lake).</p>
<p>If your Intel CPU has an integrated GPU supported by oneAPI, you can just go ahead and
install the oneAPI.jl package:</p>
<pre><code>pkg&gt; add oneAPI
</code></pre>
<p>That&rsquo;s right, no additional drivers required! oneAPI.jl ships its own copy of the <a href="https://github.com/intel/compute-runtime">Intel
Compute Runtime</a>, which works out of the box on
any (sufficiently recent) Linux kernel. The initial download, powered by Julia&rsquo;s artifact
subsystem, might take a while to complete. After that, you can import the package and start
using its functionality:</p>
<pre><code class="language-julia-repl">julia&gt; using oneAPI

julia&gt; oneAPI.versioninfo()
Binary dependencies:
- NEO_jll: 20.42.18209+0
- libigc_jll: 1.0.5186+0
- gmmlib_jll: 20.3.2+0
- SPIRV_LLVM_Translator_jll: 9.0.0+1
- SPIRV_Tools_jll: 2020.2.0+1

Toolchain:
- Julia: 1.5.2
- LLVM: 9.0.1

1 driver:
- 00007fee-06cb-0a10-1642-ca9f01000000 (v1.0.0, API v1.0.0)

1 device:
- Intel(R) Graphics Gen9
</code></pre>
<h2 id="the-onearray-type">The <code>oneArray</code> type</h2>
<p>Similar to CUDA.jl&rsquo;s <code>CuArray</code> type, oneAPI.jl provides an array abstraction that you can
use to easily perform data parallel operations on your GPU:</p>
<pre><code class="language-julia-repl">julia&gt; a = oneArray(zeros(2,3))
2×3 oneArray{Float64,2}:
 0.0  0.0  0.0
 0.0  0.0  0.0

julia&gt; a .+ 1
2×3 oneArray{Float64,2}:
 1.0  1.0  1.0
 1.0  1.0  1.0

julia&gt; sum(ans; dims=2)
2×1 oneArray{Float64,2}:
 3.0
 3.0
</code></pre>
<p>This functionality builds on the <a href="https://github.com/JuliaGPU/GPUArrays.jl/">GPUArrays.jl</a>
package, which means that a lot of operations are supported out of the box. Some are still
missing, of course, and we haven&rsquo;t carefully optimized for performance either.</p>
<h2 id="kernel-programming">Kernel programming</h2>
<p>The above array operations are made possible by a compiler that transforms Julia source code
into SPIR-V IR for use with oneAPI. Most of this work is part of
<a href="https://github.com/JuliaGPU/GPUCompiler.jl">GPUCompiler.jl</a>. In oneAPI.jl, we use this
compiler to provide a kernel programming model:</p>
<pre><code class="language-julia-repl">julia&gt; function vadd(a, b, c)
           i = get_global_id()
           @inbounds c[i] = a[i] + b[i]
           return
       end

julia&gt; a = oneArray(rand(10));

julia&gt; b = oneArray(rand(10));

julia&gt; c = similar(a);

julia&gt; @oneapi items=10 vadd(a, b, c)

julia&gt; @test Array(a) .+ Array(b) == Array(c)
Test Passed
</code></pre>
<p>Again, the <code>@oneapi</code> macro resembles <code>@cuda</code> from CUDA.jl. One of the differences with the
CUDA stack is that we use OpenCL-style built-ins, like <code>get_global_id</code> instead of
<code>threadIdx</code> and <code>barrier</code> instead of <code>sync_threads</code>. Other familiar functionality, e.g. to
reflect on the compiler, is available as well:</p>
<pre><code class="language-julia-repl">julia&gt; @device_code_spirv @oneapi vadd(a, b, c)
; CompilerJob of kernel vadd(oneDeviceArray{Float64,1,1},
;                            oneDeviceArray{Float64,1,1},
;                            oneDeviceArray{Float64,1,1})
; for GPUCompiler.SPIRVCompilerTarget

; SPIR-V
; Version: 1.0
; Generator: Khronos LLVM/SPIR-V Translator; 14
; Bound: 46
; Schema: 0
               OpCapability Addresses
               OpCapability Linkage
               OpCapability Kernel
               OpCapability Float64
               OpCapability Int64
               OpCapability Int8
          %1 = OpExtInstImport &quot;OpenCL.std&quot;
               OpMemoryModel Physical64 OpenCL
               OpEntryPoint Kernel
               ...
               OpReturn
               OpFunctionEnd
</code></pre>
<h2 id="level-zero-wrappers">Level Zero wrappers</h2>
<p>To interface with the oneAPI driver, we use the <a href="https://github.com/oneapi-src/level-zero">Level Zero
API</a>. Wrappers for this API is available under the
<code>oneL0</code> submodule of oneAPI.jl:</p>
<pre><code class="language-julia-repl">julia&gt; using oneAPI.oneL0

julia&gt; drv = first(drivers())
ZeDriver(00000000-0000-0000-1642-ca9f01000000, version 1.0.0)

julia&gt; dev = first(devices(drv))
ZeDevice(GPU, vendor 0x8086, device 0x1912): Intel(R) Graphics Gen9
</code></pre>
<p>This is a low-level interface, and importing this submodule should not be required for the
vast majority of users. It is only useful when you want to perform very specific operations,
like submitting an certain operations to the command queue, working with events, etc. In
that case, you should refer to the <a href="https://spec.oneapi.com/level-zero/latest/index.html">upstream
specification</a>; The wrappers in the
<code>oneL0</code> module closely mimic the C APIs.</p>
<h2 id="status">Status</h2>
<p>Version 0.1 of oneAPI.jl forms a solid base for future oneAPI developments in Julia. Thanks
to the continued effort of generalizing the Julia GPU support in packages like GPUArrays.jl
and GPUCompiler.jl, this initial version is already much more usable than early versions of
CUDA.jl or AMDGPU.jl ever were.</p>
<p>That said, there are crucial parts missing. For one, oneAPI.jl does not integrate with any
of the vendor libraries like oneMKL or oneDNN. That means several important operations, e.g.
matrix-matrix multiplication, will be slow. Hardware support is also limited, and the
package currently only works on Linux.</p>
<p>If you want to contribute to oneAPI.jl, or run into problems, check out the GitHub
repository at <a href="https://github.com/JuliaGPU/oneAPI.jl">JuliaGPU/oneAPI.jl</a>. For questions,
please use the <a href="https://discourse.julialang.org/c/domain/gpu">Julia Discourse forum</a> under
the GPU domain and/or in the #gpu channel of the <a href="https://julialang.org/community/">Julia
Slack</a>.</p>]]></content:encoded></item><item><title>CUDA.jl 2.1</title><link>https://juliagpu.org/2020-10-30-cuda_2.1/</link><pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-10-30-cuda_2.1/</guid><description>CUDA.jl v2.1 is a bug-fix release, with one new feature: support for cubic texture interpolations. The release also partly reverts a change from v2.0: reshape, reinterpret and contiguous views now return a CuArray again.</description><content:encoded><![CDATA[<p>CUDA.jl v2.1 is a bug-fix release, with one new feature: support for cubic texture
interpolations. The release also partly reverts a change from v2.0: <code>reshape</code>, <code>reinterpret</code>
and contiguous <code>view</code>s now return a <code>CuArray</code> again.</p>
<h2 id="generalized-texture-interpolations">Generalized texture interpolations</h2>
<p>CUDA&rsquo;s texture hardware only supports nearest-neighbour and linear interpolation, for other
modes one is required to perform the interpolation by hand. In CUDA.jl v2.1 we are
generalizing the texture interpolation API so that it is possible to use both
hardware-backed and software-implemented interpolation modes in exactly the same way:</p>
<pre><code class="language-julia"># N is the dimensionality (1, 2 or 3)
# T is the element type (needs to be supported by the texture hardware)

# source array
src = rand(T, fill(10, N)...)

# indices we want to interpolate
idx = [tuple(rand(1:0.1:10, N)...) for _ in 1:10]

# upload to the GPU
gpu_src = CuArray(src)
gpu_idx = CuArray(idx)

# create a texture array for optimized fetching
# this is required for N=1, optional for N=2 and N=3
gpu_src = CuTextureArray(gpu_src)

# interpolate using a texture
gpu_dst = CuArray{T}(undef, size(gpu_idx))
gpu_tex = CuTexture(gpu_src; interpolation=CUDA.NearestNeighbour())
broadcast!(gpu_dst, gpu_idx, Ref(gpu_tex)) do idx, tex
    tex[idx...]
end

# back to the CPU
dst = Array(gpu_dst)
</code></pre>
<p>Here, we can change the <code>interpolation</code> argument to <code>CuTexture</code> to either <code>NearestNeighbour</code>
or <code>LinearInterpolation</code>, both supported by the hardware, or <code>CubicInterpolation</code> which is
implemented in software (building on the hardware-supported linear interpolation).</p>
<h2 id="partial-revert-of-array-wrapper-changes">Partial revert of array wrapper changes</h2>
<p>In CUDA.jl v2.0, we changed the behavior of several important array operations to reuse
available wrappers in Base: <code>reshape</code> started returning a <code>ReshapedArray</code>, <code>view</code> now
returned a <code>SubArray</code>, and <code>reinterpret</code> was reworked to use <code>ReinterpretArray</code>. These
changes were made to ensure maximal compatibility with Base&rsquo;s array type, and to simplify
the implementation in CUDA.jl and GPUArrays.jl.</p>
<p>However, this change turned out to regress the time to precompile and load CUDA.jl.
Consequently, the change has been reverted, and these wrappers are now implemented as part
of the <code>CuArray</code> type again. Note however that we intend to revisit this change in the
future. It is therefore recommended to use the <code>DenseCuArray</code> type alias for methods that
need a <code>CuArray</code> backed by contiguous GPU memory. For strided <code>CuArray</code>s, i.e.
non-contiguous views, you should use the <code>StridedCuArray</code> alias.</p>]]></content:encoded></item><item><title>CUDA.jl 2.0</title><link>https://juliagpu.org/2020-10-02-cuda_2.0/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-10-02-cuda_2.0/</guid><description>Today we&amp;rsquo;re releasing CUDA.jl 2.0, a breaking release with several new features. Highlights include initial support for Float16, a switch to CUDA&amp;rsquo;s new stream model, a much-needed rework of the sparse array support and support for CUDA 11.1.
The release now requires Julia 1.5, and assumes a GPU with compute capability 5.0 or higher (although most of the package will still work with an older GPU).</description><content:encoded><![CDATA[<p>Today we&rsquo;re releasing CUDA.jl 2.0, a breaking release with several new features. Highlights
include initial support for Float16, a switch to CUDA&rsquo;s new stream model, a much-needed
rework of the sparse array support and support for CUDA 11.1.</p>
<p>The release now requires <strong>Julia 1.5</strong>, and assumes a GPU with <strong>compute capability 5.0</strong> or
higher (although most of the package will still work with an older GPU).</p>
<h2 id="low--and-mixed-precision-operations">Low- and mixed-precision operations</h2>
<p>With NVIDIA&rsquo;s latest GPUs featuring more and more low-precision operations,
CUDA.jl <a href="https://github.com/JuliaGPU/CUDA.jl/pull/417">now</a> starts to support
these data types. For example, the CUBLAS wrappers can be used with (B)Float16
inputs (running under <code>JULIA_DEBUG=CUBLAS</code> to illustrate the called methods)
thanks to the <code>cublasGemmEx</code> API call:</p>
<pre><code class="language-julia-repl">julia&gt; mul!(CUDA.zeros(Float32,2,2),
            cu(rand(Float16,2,2)),
            cu(rand(Float16,2,2)))

I! cuBLAS (v11.0) function cublasStatus_t cublasGemmEx(...) called:
i!  Atype: type=cudaDataType_t; val=CUDA_R_16F(2)
i!  Btype: type=cudaDataType_t; val=CUDA_R_16F(2)
i!  Ctype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_32F(68)

2×2 CuArray{Float32,2}:
 0.481284  0.561241
 1.12923   1.04541
</code></pre>
<pre><code class="language-julia-repl">julia&gt; using BFloat16s

julia&gt; mul!(CUDA.zeros(BFloat16,2,2),
            cu(BFloat16.(rand(2,2))),
            cu(BFloat16.(rand(2,2))))

I! cuBLAS (v11.0) function cublasStatus_t cublasGemmEx(...) called:
i!  Atype: type=cudaDataType_t; val=CUDA_R_16BF(14)
i!  Btype: type=cudaDataType_t; val=CUDA_R_16BF(14)
i!  Ctype: type=cudaDataType_t; val=CUDA_R_16BF(14)
i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_32F(68)

2×2 CuArray{BFloat16,2}:
 0.300781   0.71875
 0.0163574  0.0241699
</code></pre>
<p>Alternatively, CUBLAS can be configured to automatically down-cast 32-bit inputs to Float16.
This is <a href="https://github.com/JuliaGPU/CUDA.jl/pull/424">now</a> exposed through a task-local
CUDA.jl math mode:</p>
<pre><code class="language-julia-repl">julia&gt; CUDA.math_mode!(CUDA.FAST_MATH; precision=:Float16)

julia&gt; mul!(CuArray(zeros(Float32,2,2)),
            CuArray(rand(Float32,2,2)),
            CuArray(rand(Float32,2,2)))

I! cuBLAS (v11.0) function cublasStatus_t cublasGemmEx(...) called:
i!  Atype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  Btype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  Ctype: type=cudaDataType_t; val=CUDA_R_32F(0)
i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_32F_FAST_16F(74)

2×2 CuArray{Float32,2}:
 0.175258  0.226159
 0.511893  0.331351
</code></pre>
<p>As part of these changes, CUDA.jl now defaults to using tensor cores. This may affect
accuracy; use math mode <code>PEDANTIC</code> if you want the old behavior.</p>
<p>Work is <a href="https://github.com/JuliaGPU/CUDA.jl/issues/391">under way</a> to extend these
capabilities to the rest of CUDA.jl, e.g., the CUDNN wrappers, or the native kernel
programming capabilities.</p>
<h2 id="new-default-stream-semantics">New default stream semantics</h2>
<p>In CUDA.jl 2.0 we&rsquo;re <a href="https://github.com/JuliaGPU/CUDA.jl/pull/395">switching</a> to CUDA&rsquo;s
<a href="https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/">simplified stream programming
model</a>.
This simplifies working with multiple streams, and opens up more possibilities for
concurrent execution of GPU operations.</p>
<h3 id="multi-stream-programming">Multi-stream programming</h3>
<p>In the old model, the default stream (used by all GPU operations unless specified otherwise)
was a special stream whose commands could not be executed concurrently with commands on
regular, explicitly-created streams. For example, if we interleave kernels executed on a
dedicated stream with ones on the default one, execution was serialized:</p>
<pre><code class="language-julia">using CUDA

N = 1 &lt;&lt; 20

function kernel(x, n)
    tid = threadIdx().x + (blockIdx().x-1) * blockDim().x
    for i = tid:blockDim().x*gridDim().x:n
        x[i] = CUDA.sqrt(CUDA.pow(3.14159f0, i))
    end
    return
end

num_streams = 8

for i in 1:num_streams
    stream = CuStream()

    data = CuArray{Float32}(undef, N)

    @cuda blocks=1 threads=64 stream=stream kernel(data, N)

    @cuda kernel(data, 0)
end
</code></pre>


<figure>
	<img src="https://juliagpu.org/2020-10-02-cuda_2.0/multistream_before.png" alt="Multi-stream programming (old)" />
</figure>

<p>In the new model, default streams are regular streams and commands issued on them can
execute concurrently with those on other streams:</p>


<figure>
	<img src="https://juliagpu.org/2020-10-02-cuda_2.0/multistream_after.png" alt="Multi-stream programming (new)" />
</figure>

<h3 id="multi-threading">Multi-threading</h3>
<p>Another consequence of the new stream model is that each thread gets its own default stream
(accessible as <code>CuStreamPerThread()</code>). Together with Julia&rsquo;s threading capabilities, this
makes it trivial to group independent work in tasks, benefiting from concurrent execution on
the GPU where possible:</p>
<pre><code class="language-julia">using CUDA

N = 1 &lt;&lt; 20

function kernel(x, n)
    tid = threadIdx().x + (blockIdx().x-1) * blockDim().x
    for i = tid:blockDim().x*gridDim().x:n
        x[i] = CUDA.sqrt(CUDA.pow(3.14159f0, i))
    end
    return
end

Threads.@threads for i in 1:Threads.nthreads()
    data = CuArray{Float32}(undef, N)
    @cuda blocks=1 threads=64 kernel(data, N)
    synchronize(CuDefaultStream())
end
</code></pre>


<figure>
	<img src="https://juliagpu.org/2020-10-02-cuda_2.0/multithread_after.png" alt="Multi-threading (new)" />
</figure>

<p>With the old model, execution would have been serialized because the default stream was the
same across threads:</p>


<figure>
	<img src="https://juliagpu.org/2020-10-02-cuda_2.0/multithread_before.png" alt="Multi-threading (old)" />
</figure>

<p>Future improvements will make this behavior configurable, such that users can use a
different default stream per task.</p>
<h2 id="sparse-array-clean-up">Sparse array clean-up</h2>
<p>As part of CUDA.jl 2.0, the sparse array support <a href="https://github.com/JuliaGPU/CUDA.jl/pull/409">has been
refactored</a>, bringing them in line with other
array types and their expected behavior. For example, the custom <code>switch2</code> methods have been
removed in favor of calls to <code>convert</code> and array constructors:</p>
<pre><code class="language-julia-repl">julia&gt; using SparseArrays
julia&gt; using CUDA, CUDA.CUSPARSE

julia&gt; CuSparseMatrixCSC(CUDA.rand(2,2))
2×2 CuSparseMatrixCSC{Float32} with 4 stored entries:
  [1, 1]  =  0.124012
  [2, 1]  =  0.791714
  [1, 2]  =  0.487905
  [2, 2]  =  0.752466

julia&gt; CuSparseMatrixCOO(sprand(2,2, 0.5))
2×2 CuSparseMatrixCOO{Float64} with 3 stored entries:
  [1, 1]  =  0.183183
  [2, 1]  =  0.966466
  [2, 2]  =  0.064101

julia&gt; CuSparseMatrixCSR(ans)
2×2 CuSparseMatrixCSR{Float64} with 3 stored entries:
  [1, 1]  =  0.183183
  [2, 1]  =  0.966466
  [2, 2]  =  0.064101
</code></pre>
<p><a href="https://github.com/JuliaGPU/CUDA.jl/pull/421">Initial support for the COO sparse matrix type
</a> has also been added, along with more <a href="https://github.com/JuliaGPU/CUDA.jl/pull/351">better
support for sparse matrix-vector
multiplication</a>.</p>
<h2 id="support-for-cuda-111">Support for CUDA 11.1</h2>
<p>This release also features support for the brand-new CUDA 11.1. As there is no compatible
release of CUDNN or CUTENSOR yet, CUDA.jl won&rsquo;t automatically select this version, but you
can force it to by setting the <code>JULIA_CUDA_VERSION</code> environment variable to <code>11.1</code>:</p>
<pre><code class="language-julia-repl">julia&gt; ENV[&quot;JULIA_CUDA_VERSION&quot;] = &quot;11.1&quot;

julia&gt; using CUDA

julia&gt; CUDA.versioninfo()
CUDA toolkit 11.1.0, artifact installation

Libraries:
- CUDNN: missing
- CUTENSOR: missing
</code></pre>
<h2 id="minor-changes">Minor changes</h2>
<p>Many other changes are part of this release:</p>
<ul>
<li>Views, reshapes and array reinterpretations <a href="https://github.com/JuliaGPU/CUDA.jl/pull/437">are now
represented</a> by the Base array wrappers,
simplifying the CuArray type definition.</li>
<li>Various optimizations to <a href="https://github.com/JuliaGPU/CUDA.jl/pull/428">CUFFT</a> and
<a href="https://github.com/JuliaGPU/CUDA.jl/pull/321">CUDNN</a> library wrappers.</li>
<li><a href="https://github.com/JuliaGPU/CUDA.jl/pull/427">Support</a> for <code>LinearAlgebra.reflect!</code> and
<code>rotate!</code></li>
<li><a href="https://github.com/JuliaGPU/CUDA.jl/pull/435">Initial support</a> for calling CUDA libraries
with strided inputs</li>
</ul>]]></content:encoded></item><item><title>Paper: Flexible Performant GEMM Kernels on GPUs</title><link>https://juliagpu.org/2020-09-28-gemmkernels/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-09-28-gemmkernels/</guid><description>General Matrix Multiplication or GEMM kernels take center place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA&amp;rsquo;s Tensor Cores. In this paper we show how it is possible to program these accelerators from Julia, and present abstractions and interfaces that allow to do so efficiently without sacrificing performance.</description><content:encoded><![CDATA[<p>General Matrix Multiplication or GEMM kernels take center place in high performance
computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as
NVIDIA&rsquo;s Tensor Cores. In this paper we show how it is possible to program these
accelerators from Julia, and present abstractions and interfaces that allow to do so
efficiently without sacrificing performance.</p>
<p>A pre-print of the paper has been published on arXiv:
<a href="https://arxiv.org/abs/2009.12263">arXiv:2009.12263</a>. <br> The source code can be found on
GitHub:
<a href="https://github.com/thomasfaingnaert/GemmKernels.jl">thomasfaingnaert/GemmKernels.jl</a>.</p>
<p>With the APIs from GemmKernels.jl, it is possible to instantiate GEMM kernels that perform
in the same ball park as, and sometimes even outperform state-of-the-art libraries like
CUBLAS and CUTLASS. For example, performing a mixed-precision multiplication of two 16-bit
matrixes into a 32-bit accumulator (on different combinations of layouts):</p>


<figure>
	<img src="https://juliagpu.org/2020-09-28-gemmkernels/mixed_precision.png" alt="Performance of mixed-precision GEMM" />
</figure>

<p>The APIs are also highly flexible and allow customization of each step, e.g., to apply the
activation function <code>max(x, 0)</code> for implementing a rectified linear unit (ReLU):</p>
<pre><code class="language-julia">a = CuArray(rand(Float16, (M, K)))
b = CuArray(rand(Float16, (K, N)))
c = CuArray(rand(Float32, (M, N)))
d = similar(c)

conf = GemmKernels.get_config(
    gemm_shape = (M = M, N = N, K = K),
    operator = Operator.WMMAOp{16, 16, 16},
    global_a_layout = Layout.AlignedColMajor{Float16},
    global_c_layout = Layout.AlignedColMajor{Float32})

GemmKernels.matmul(
    a, b, c, d, conf;
    transform_regs_to_shared_d = Transform.Elementwise(x -&gt; max(x, 0)))
</code></pre>
<p>The GemmKernels.jl framework is written entirely in Julia, demonstrating the
high-performance GPU programming capabilities of this language, but at the same time keeping
the research accessible and easy to modify or repurpose by other Julia developers.</p>]]></content:encoded></item><item><title>CUDA.jl 1.3 - Multi-device programming</title><link>https://juliagpu.org/2020-07-18-cuda_1.3/</link><pubDate>Sat, 18 Jul 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-07-18-cuda_1.3/</guid><description>Today we&amp;rsquo;re releasing CUDA.jl 1.3, with several new features. The most prominent change is support for multiple GPUs within a single process.</description><content:encoded><![CDATA[<p>Today we&rsquo;re releasing CUDA.jl 1.3, with several new features. The most prominent
change is support for multiple GPUs within a single process.</p>
<h2 id="multi-gpu-programming">Multi-GPU programming</h2>
<p>With CUDA.jl 1.3, you can finally use multiple CUDA GPUs within a single process. To switch
devices you can call <code>device!</code>, query the current device with <code>device()</code>, or reset it using
<code>device_reset!()</code>:</p>
<pre><code class="language-julia-repl">julia&gt; collect(devices())
9-element Array{CuDevice,1}:
 CuDevice(0): Tesla V100-PCIE-32GB
 CuDevice(1): Tesla V100-PCIE-32GB
 CuDevice(2): Tesla V100-PCIE-32GB
 CuDevice(3): Tesla V100-PCIE-32GB
 CuDevice(4): Tesla V100-PCIE-16GB
 CuDevice(5): Tesla P100-PCIE-16GB
 CuDevice(6): Tesla P100-PCIE-16GB
 CuDevice(7): GeForce GTX 1080 Ti
 CuDevice(8): GeForce GTX 1080 Ti

julia&gt; device!(5)

julia&gt; device()
CuDevice(5): Tesla P100-PCIE-16GB
</code></pre>
<p>Let&rsquo;s define a kernel to show this really works:</p>
<pre><code class="language-julia-repl">julia&gt; function kernel()
           dev = Ref{Cint}()
           CUDA.cudaGetDevice(dev)
           @cuprintln(&quot;Running on device $(dev[])&quot;)
           return
       end

julia&gt; @cuda kernel()
Running on device 5

julia&gt; device!(0)

julia&gt; device()
CuDevice(0): Tesla V100-PCIE-32GB

julia&gt; @cuda kernel()
Running on device 0
</code></pre>
<p>Memory allocations, like <code>CuArray</code>s, are implicitly bound to the device they
were allocated on. That means you should take care to only use an array when the
owning device is active, or you will run into errors:</p>
<pre><code class="language-julia-repl">julia&gt; device()
CuDevice(0): Tesla V100-PCIE-32GB

julia&gt; a = CUDA.rand(1)
1-element CuArray{Float32,1}:
 0.6322775

julia&gt; device!(1)

julia&gt; a
ERROR: CUDA error: an illegal memory access was encountered
</code></pre>
<p>Future improvements might make the array type device-aware.</p>
<h2 id="multitasking-and-multithreading">Multitasking and multithreading</h2>
<p>Dovetailing with the support for multiple GPUs, is the ability to use these GPUs
on separate Julia tasks and threads:</p>
<pre><code class="language-julia-repl">julia&gt; device!(0)

julia&gt; @sync begin
         @async begin
           device!(1)
           println(&quot;Working with $(device()) on $(current_task())&quot;)
           yield()
           println(&quot;Back to device $(device()) on $(current_task())&quot;)
         end
         @async begin
           device!(2)
           println(&quot;Working with $(device()) on $(current_task())&quot;)
         end
       end
Working with CuDevice(1) on Task @0x00007fc9e6a48010
Working with CuDevice(2) on Task @0x00007fc9e6a484f0
Back to device CuDevice(1) on Task @0x00007fc9e6a48010

julia&gt; device()
CuDevice(0): Tesla V100-PCIE-32GB
</code></pre>
<p>Each task has its own local GPU state, such as the device it was bound to,
handles to libraries like CUBLAS or CUDNN (which means that each task can
configure libraries independently), etc.</p>
<h2 id="minor-features">Minor features</h2>
<p>CUDA.jl 1.3 also features some minor changes:</p>
<ul>
<li>Reinstated compatibility with Julia 1.3</li>
<li>Support for CUDA 11.0 Update 1</li>
<li>Support for CUDNN 8.0.2</li>
</ul>
<h2 id="known-issues">Known issues</h2>
<p>Several operations on sparse arrays have been broken since CUDA.jl 1.2, due to
the deprecations that were part of CUDA 11. The next version of CUDA.jl will
drop support for CUDA 10.0 or older, which will make it possible to use new
cuSPARSE APIs and add back missing functionality.</p>]]></content:encoded></item><item><title>CUDA.jl 1.1</title><link>https://juliagpu.org/2020-07-07-cuda_1.1/</link><pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-07-07-cuda_1.1/</guid><description>CUDA.jl 1.1 marks the first feature release after merging several CUDA packages into one. It raises the minimal Julia version to 1.4, and comes with support for the impending 1.5 release.</description><content:encoded><![CDATA[<p>CUDA.jl 1.1 marks the first feature release after merging several CUDA packages into one. It
raises the minimal Julia version to 1.4, and comes with support for the impending 1.5
release.</p>
<h2 id="cudajl-replacing-cuarrayscudanativejl">CUDA.jl replacing CuArrays/CUDAnative.jl</h2>
<p>As <a href="https://discourse.julialang.org/t/psa-cuda-jl-replacing-cuarrays-jl-cudanative-jl-cudadrv-jl-cudaapi-jl-call-for-testing/40205">announced a while
back</a>,
CUDA.jl is now the new package for programming CUDA GPUs in Julia, replacing CuArrays.jl,
CUDAnative.jl, CUDAdrv.jl and CUDAapi.jl. The merged package should be a drop-in
replacement: All existing functionality has been ported, and almost all exported functions
are still there. Applications like Flux.jl or the DiffEq.jl stack are being updated to
support this change.</p>
<h2 id="cuda-11-support">CUDA 11 support</h2>
<p>With CUDA.jl 1.1, we support the upcoming release of the CUDA toolkit. This only applies to
locally-installed versions of the toolkit, i.e., you need to specify
<code>JULIA_CUDA_USE_BINARYBUILDER=false</code> in your environment to pick up the locally-installed
release candidate of the CUDA toolkit. New features, like the third-generation tensor cores
and its extended type support, or any new APIs, are not yet natively supported by Julia
code.</p>
<h2 id="nvidia-management-library-nvml">NVIDIA Management Library (NVML)</h2>
<p>CUDA.jl now integrates with the NVIDIA Management Library, or NVML. With this library, it&rsquo;s
possible to query information about the system, any GPU devices, their topology, etc.:</p>
<pre><code class="language-julia-repl">julia&gt; using CUDA

julia&gt; dev = first(NVML.devices())
CUDA.NVML.Device(Ptr{Nothing} @0x00007f987c7c6e38)

julia&gt; NVML.uuid(dev)
UUID(&quot;b8d5e790-ea4d-f962-e0c3-0448f69f2e23&quot;)

julia&gt; NVML.name(dev)
&quot;Quadro RTX 5000&quot;

julia&gt; NVML.power_usage(dev)
37.863

julia&gt; NVML.energy_consumption(dev)
65330.292
</code></pre>
<h2 id="experimental-texture-support">Experimental: Texture support</h2>
<p>It is now also possible to use the GPU&rsquo;s hardware texture support from Julia, albeit using a
fairly low-level and still experimental API (many thanks to
<a href="https://github.com/cdsousa">@cdsousa</a> for the initial development). As a demo, let&rsquo;s start
with loading a sample image:</p>
<pre><code class="language-julia">julia&gt; using Images, TestImages, ColorTypes, FixedPointNumbers
julia&gt; img = RGBA{N0f8}.(testimage(&quot;lighthouse&quot;))
</code></pre>
<p>We use RGBA since CUDA&rsquo;s texture hardware only supports 1, 2 or 4 channels. This support is
also currently limited to &ldquo;plain&rdquo; types, so let&rsquo;s reinterpret the image:</p>
<pre><code class="language-julia">julia&gt; img′ = reinterpret(NTuple{4,UInt8}, img)
</code></pre>
<p>Now we can upload this image to the array, using the <code>CuTextureArray</code> type for optimized
storage (normal <code>CuArray</code>s are supported too), and bind it to a <code>CuTexture</code> object that we
can pass to a kernel:</p>
<pre><code class="language-julia-repl">julia&gt; texturearray = CuTextureArray(img′)

julia&gt; texture = CuTexture(texturearray; normalized_coordinates=true)
512×768 4-channel CuTexture(::CuTextureArray) with eltype NTuple{4,UInt8}
</code></pre>
<p>Let&rsquo;s write and a kernel that warps this image. Since we specified
<code>normalized_coordinates=true</code>, we index the texture using values in <code>[0,1]</code>:</p>
<pre><code class="language-julia">function warp(dst, texture)
    tid = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    I = CartesianIndices(dst)
    @inbounds if tid &lt;= length(I)
        i,j = Tuple(I[tid])
        u = Float32(i-1) / Float32(size(dst, 1)-1)
        v = Float32(j-1) / Float32(size(dst, 2)-1)
        x = u + 0.02f0 * CUDA.sin(30v)
        y = v + 0.03f0 * CUDA.sin(20u)
        dst[i,j] = texture[x,y]
    end
    return
end
</code></pre>
<p>The size of the output image determines how many elements we need to process. This needs to
be translated to a number of threads and blocks, keeping in mind device and kernel
characteristics. We automate this using the occupancy API:</p>
<pre><code class="language-julia-repl">julia&gt; outimg_d = CuArray{eltype(img′)}(undef, 500, 1000);

julia&gt; function configurator(kernel)
           config = launch_configuration(kernel.fun)

           threads = Base.min(length(outimg_d), config.threads)
           blocks = cld(length(outimg_d), threads)

           return (threads=threads, blocks=blocks)
       end

julia&gt; @cuda config=configurator warp(outimg_d, texture)
</code></pre>
<p>Finally, we fetch and visualize the output:</p>
<pre><code class="language-julia-repl">julia&gt; outimg = Array(outimg_d)

julia&gt; save(&quot;imgwarp.png&quot;, reinterpret(eltype(img), outimg))
</code></pre>


<figure>
	<img src="https://juliagpu.org/2020-07-07-cuda_1.1/imgwarp.png" alt="Warped lighthouse" />
</figure>

<h2 id="minor-features">Minor features</h2>
<p>The test-suite is now parallelized, using up-to <code>JULIA_NUM_THREADS</code> processes:</p>
<pre><code>$ JULIA_NUM_THREADS=4 julia -e 'using Pkg; Pkg.test(&quot;CUDA&quot;);'

                                     |          | ---------------- GPU ---------------- | ---------------- CPU ---------------- |
Test                        (Worker) | Time (s) | GC (s) | GC % | Alloc (MB) | RSS (MB) | GC (s) | GC % | Alloc (MB) | RSS (MB) |
initialization                   (2) |     2.52 |   0.00 |  0.0 |       0.00 |   115.00 |   0.05 |  1.8 |     153.13 |   546.27 |
apiutils                         (4) |     0.55 |   0.00 |  0.0 |       0.00 |   115.00 |   0.02 |  4.0 |      75.86 |   522.36 |
codegen                          (4) |    14.81 |   0.36 |  2.5 |       0.00 |   157.00 |   0.62 |  4.2 |    1592.28 |   675.15 |
...
gpuarrays/mapreduce essentials   (2) |   113.52 |   0.01 |  0.0 |       3.19 |   641.00 |   2.61 |  2.3 |    8232.84 |  2449.35 |
gpuarrays/mapreduce (old tests)  (5) |   138.35 |   0.01 |  0.0 |     130.20 |   507.00 |   2.94 |  2.1 |    8615.15 |  2353.62 |
gpuarrays/mapreduce derivatives  (3) |   180.52 |   0.01 |  0.0 |       3.06 |   229.00 |   3.44 |  1.9 |   12262.67 |  1403.39 |

Test Summary: |  Pass  Broken  Total
  Overall     | 11213       3  11216
    SUCCESS
    Testing CUDA tests passed
</code></pre>
<p>A copy of <code>Base.versioninfo()</code> is available to report on the CUDA toolchain and any devices:</p>
<pre><code class="language-julia-repl">julia&gt; CUDA.versioninfo()
CUDA toolkit 10.2.89, artifact installation
CUDA driver 11.0.0
NVIDIA driver 450.36.6

Libraries:
- CUBLAS: 10.2.2
- CURAND: 10.1.2
- CUFFT: 10.1.2
- CUSOLVER: 10.3.0
- CUSPARSE: 10.3.1
- CUPTI: 12.0.0
- NVML: 11.0.0+450.36.6
- CUDNN: 7.6.5 (for CUDA 10.2.0)
- CUTENSOR: 1.1.0 (for CUDA 10.2.0)

Toolchain:
- Julia: 1.5.0-rc1.0
- LLVM: 9.0.1
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4
- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75

1 device(s):
- Quadro RTX 5000 (sm_75, 14.479 GiB / 15.744 GiB available)
</code></pre>
<p>CUTENSOR artifacts have been upgraded to version 1.1.0.</p>
<p>Benchmarking infrastructure based on the Codespeed project has been set-up at
<a href="https://speed.juliagpu.org/">speed.juliagpu.org</a> to keep track of the performance of
various operations.</p>]]></content:encoded></item><item><title>CUDAnative.jl 3.0 and CuArrays.jl 2.0</title><link>https://juliagpu.org/cudanative_3.0-cuarrays_2.0/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/cudanative_3.0-cuarrays_2.0/</guid><description>This release of the Julia CUDA stack contains some exciting new features: automatic installation of CUDA using artifacts, full support for GPU method redefinitions, and experimental support for multitasking and multithreading. The release is technically breaking, but most end-users should not be affected.</description><content:encoded><![CDATA[<p>This release of the Julia CUDA stack contains some exciting new features: automatic
installation of CUDA using artifacts, full support for GPU method redefinitions, and
experimental support for multitasking and multithreading. The release is technically
breaking, but most end-users should not be affected.</p>
<h2 id="api-changes">API changes</h2>
<p>Changes to certain APIs require these releases to be breaking, however, most users should
not be affected and chances are you can just bump your Compat entries without any additional
changes. Flux.jl users will have to wait a little longer though, as the package uses
non-public APIs that have changed and <a href="https://github.com/FluxML/Flux.jl/pull/1050">requires an
update</a>.</p>
<h2 id="artifacts">Artifacts</h2>
<p>CUDA and its dependencies will now be automatically installed using artifacts generated by
BinaryBuilder.jl. This greatly improves usability, and only requires a functioning NVIDIA
driver:</p>
<pre><code class="language-julia-repl">julia&gt; ENV[&quot;JULIA_DEBUG&quot;] = &quot;CUDAnative&quot;

julia&gt; using CUDAnative

julia&gt; CUDAnative.version()
┌ Debug: Trying to use artifacts...
└ @ CUDAnative CUDAnative/src/bindeps.jl:52
┌ Debug: Using CUDA 10.2.89 from an artifact at /depot/artifacts/...
└ @ CUDAnative CUDAnative/src/bindeps.jl:108
v&quot;10.2.89&quot;
</code></pre>
<p>Use of a local installation is still possible by setting the environment variable
<code>JULIA_CUDA_USE_BINARYBUILDER</code> to false. For more details, refer to <a href="https://juliagpu.gitlab.io/CUDA.jl/installation/overview/">the
documentation</a>.</p>
<p>Relevant PRs: <a href="https://github.com/JuliaGPU/CUDAnative.jl/pull/492">CUDAnative.jl#492</a> and <a href="https://github.com/JuliaGPU/CuArrays.jl/pull/490">CuArrays.jl#490</a></p>
<h2 id="method-redefinitions">Method redefinitions</h2>
<p>CUDAnative 3.0 now fully supports method redefinitions, commonly referred to as <a href="https://github.com/JuliaLang/julia/issues/265">Julia
issue #265</a>, and makes it possible to use
interactive programming tools like Revise.jl:</p>
<pre><code class="language-julia-repl">julia&gt; child() = 0
julia&gt; parent() = (@cuprintln(child()); return)
julia&gt; @cuda parent()
0

julia&gt; parent() = (@cuprintln(child() + 1); return)
julia&gt; @cuda parent()
1


julia&gt; child() = 1
julia&gt; @cuda parent()
2
</code></pre>
<p>Relevant PRs: <a href="https://github.com/JuliaGPU/CUDAnative.jl/pull/581">CUDAnative.jl#581</a></p>
<h2 id="experimental-multitasking-and-multithreading">Experimental: Multitasking and multithreading</h2>
<p>With CUDAnative 3.0 and CuArrays 2.0 you can now use Julia tasks and threads to organize
your code. In combination with CUDA streams, this makes it possible to execute kernels and
other GPU operations in parallel:</p>
<pre><code class="language-julia">@sync begin
    function my_expensive_kernel()
        return
    end
    @async @cuda stream=CuStream() my_expensive_kernel()
    @async @cuda stream=CuStream() my_expensive_kernel()
end
</code></pre>
<p>Every task, whether it runs on a separate thread or not, can work with a different
device, as well as independently work with CUDA libraries like CUBLAS and CUFFT.</p>
<p>Note that this support is experimental, and lacks certain features to be fully effective.
For one, the CuArrays memory allocator is not device-aware, and it is currently not possible
to configure the CUDA stream for operations like map or broadcast.</p>
<p>Relevant PRs: <a href="https://github.com/JuliaGPU/CUDAnative.jl/pull/609">CUDAnative.jl#609</a> and
<a href="https://github.com/JuliaGPU/CuArrays.jl/pull/645">CuArrays.jl#645</a></p>
<h2 id="minor-changes">Minor changes</h2>
<p>GPU kernels are now name-mangled like C++, which offers better integration with NVIDIA tools
(<a href="https://github.com/JuliaGPU/CUDAnative.jl/pull/559">CUDAnative.jl#559</a>).</p>
<p>A better N-dimensional <code>mapreducedim!</code> kernel, properly integrating with all Base interfaces
(<a href="https://github.com/JuliaGPU/CuArrays.jl/pull/602">CuArrays.jl#602</a> and
<a href="https://github.com/JuliaGPU/GPUArrays.jl/pull/246">GPUArrays#246</a>).</p>
<p>A <code>CuIterator</code> type for batching arrays to the GPU (by @jrevels,
<a href="https://github.com/JuliaGPU/CuArrays.jl/pull/467">CuArrays.jl#467</a>).</p>
<p>Integration with Base&rsquo;s 5-arg <code>mul!</code> (by @haampie,
<a href="https://github.com/JuliaGPU/CuArrays.jl/pull/641">CuArrays.jl#641</a> and
<a href="https://github.com/JuliaGPU/GPUArrays.jl/pull/253">GPUArrays#253</a>).</p>
<p>Integration with Cthulhu.jl for interactive inspection of generated code
(<a href="https://github.com/JuliaGPU/CUDAnative.jl/issues/597">CUDAnative.jl#597</a>).</p>
<h2 id="known-issues">Known issues</h2>
<p>With a release as big as this one there&rsquo;s bound to be some bugs, e.g., with the
installation of artifacts on exotic systems, or due to the many changes to make the
libraries thread-safe. If you need absolute stability, please wait for a point release.</p>
<p>There are also some known issues. CUDAnative is currently not compatible with Julia 1.5 due
to Base compiler changes (<a href="https://github.com/JuliaLang/julia/issues/34993">julia#34993</a>),
the new <code>mapreducedim!</code> kernel appears to be slower in some cases
(<a href="https://github.com/JuliaGPU/CuArrays.jl/issues/611">CuArrays.jl#611</a>), and there are some
remaining thread-safety issues when using the non-default memory pool
(<a href="https://github.com/JuliaGPU/CuArrays.jl/issues/647">CuArrays.jl#647</a>).</p>]]></content:encoded></item><item><title>Julia's Dramatic Rise in HPC and Elsewhere</title><link>https://juliagpu.org/2020-01-14-hpcwire/</link><pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-01-14-hpcwire/</guid></item><item><title>Accelerating Tensor Computations in Julia with the GPU</title><link>https://juliagpu.org/2020-01-05-itensors/</link><pubDate>Sun, 05 Jan 2020 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2020-01-05-itensors/</guid></item><item><title>New website for JuliaGPU</title><link>https://juliagpu.org/new_site/</link><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliagpu.org/new_site/</guid><description>Welcome to the new landing page for the JuliaGPU organization. This website serves as an introduction to the several packages for programming GPUs in Julia, with pointers to relevant resources for new users.</description><content:encoded><![CDATA[<p>Welcome to the new landing page for the JuliaGPU organization. This website serves as an
introduction to the several packages for programming GPUs in Julia, with pointers to
relevant resources for new users.</p>
<p>The sources for this website are hosted at
<a href="https://github.com/JuliaGPU/juliagpu.org">GitHub</a> and generated using Hugo, feel free to
open an issue or pull request if you think it could be improved.</p>]]></content:encoded></item><item><title>DifferentialEquations.jl v6.9.0 released with automatic Multi-GPU support</title><link>https://juliagpu.org/2019-12-03-diffeq-0.6.9/</link><pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2019-12-03-diffeq-0.6.9/</guid></item><item><title>Julia Computing Brings Support for NVIDIA GPU Computing on Arm Powered Servers</title><link>https://juliagpu.org/2019-12-03-nvidia-arm-gpu/</link><pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2019-12-03-nvidia-arm-gpu/</guid></item><item><title>An Introduction to GPU Programming in Julia</title><link>https://juliagpu.org/2019-10-17-nextjournal-intro/</link><pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2019-10-17-nextjournal-intro/</guid></item><item><title>DifferentialEquations.jl v6.4.0 released with full GPU ODE support</title><link>https://juliagpu.org/2019-05-09-diffeq-0.6.4/</link><pubDate>Thu, 09 May 2019 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2019-05-09-diffeq-0.6.4/</guid></item><item><title>Next Generation Climate Models leverage Julia and GPUs</title><link>https://juliagpu.org/2019-03-04-nps-clima-gpu/</link><pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2019-03-04-nps-clima-gpu/</guid></item><item><title>New Climate Model to be Built from the Ground Up</title><link>https://juliagpu.org/2018-12-12-paoc-clima/</link><pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2018-12-12-paoc-clima/</guid></item><item><title>Solving Systems of Stochastic PDEs and using GPUs in Julia</title><link>https://juliagpu.org/2017-12-11-diffeq-stochastic_pdes/</link><pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2017-12-11-diffeq-stochastic_pdes/</guid></item><item><title>High-Performance GPU Computing in the Julia Programming Language</title><link>https://juliagpu.org/2017-10-25-nvidia-devblog/</link><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid>https://juliagpu.org/2017-10-25-nvidia-devblog/</guid></item><item><title>About</title><link>https://juliagpu.org/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliagpu.org/about/</guid><description>Hugo is the world’s fastest framework for building websites. It is written in Go.
It makes use of a variety of open source projects including:
https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Learn more and contribute on GitHub.</description><content:encoded><![CDATA[<p>Hugo is the <strong>world’s fastest framework for building websites</strong>. It is written in Go.</p>
<p>It makes use of a variety of open source projects including:</p>
<ul>
<li><a href="https://github.com/russross/blackfriday">https://github.com/russross/blackfriday</a></li>
<li><a href="https://github.com/alecthomas/chroma">https://github.com/alecthomas/chroma</a></li>
<li><a href="https://github.com/muesli/smartcrop">https://github.com/muesli/smartcrop</a></li>
<li><a href="https://github.com/spf13/cobra">https://github.com/spf13/cobra</a></li>
<li><a href="https://github.com/spf13/viper">https://github.com/spf13/viper</a></li>
</ul>
<p>Learn more and contribute on <a href="https://github.com/gohugoio">GitHub</a>.</p>
]]></content:encoded></item><item><title>AMD ROCm</title><link>https://juliagpu.org/rocm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliagpu.org/rocm/</guid><description>The Julia programming support for AMD GPUs based on the ROCm platform aims to provide similar capabilities as the NVIDIA CUDA stack, with support for both low-level kernel programming as well as an array-oriented interface.
Julia support exists in the form of a single package:
AMDGPU.jl This package contains everything needed to access the HSA runtime, program GPU kernels, and utilize a user-friendly array-based interface. The stack originally was divided into 3 separate packages, which still exist and may be of use for interested users and developers:</description><content:encoded><![CDATA[<p><a href="https://juliagpu.gitlab.io/AMDGPU.jl/"><img src="https://img.shields.io/badge/docs-latest-blue.svg" alt=""></a> <a href="https://github.com/JuliaGPU/AMDGPU.jl"><img src="https://img.shields.io/github/stars/JuliaGPU/AMDGPU.jl?style=social" alt=""></a></p>
<p>The Julia programming support for AMD GPUs based on the ROCm platform aims to
provide similar capabilities as the <a href="/cuda/">NVIDIA CUDA</a> stack, with support
for both low-level kernel programming as well as an array-oriented interface.</p>
<p>Julia support exists in the form of a single package:</p>
<ul>
<li><a href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a></li>
</ul>
<p>This package contains everything needed to access the HSA runtime, program GPU
kernels, and utilize a user-friendly array-based interface. The stack
originally was divided into 3 separate packages, which still exist and may be
of use for interested users and developers:</p>
<ul>
<li><a href="https://github.com/JuliaGPU/HSARuntime.jl">HSARuntime.jl</a>: interfacing with the HSA runtime</li>
<li><a href="https://github.com/JuliaGPU/AMDGPUnative.jl">AMDGPUnative.jl</a>: GPU kernel programming support</li>
<li><a href="https://github.com/JuliaGPU/ROCArrays.jl">ROCArrays.jl</a>: array programming interface</li>
</ul>
<p>At this point, the toolchain is a work in progress, although it is quite
functional for simple usecases. We only officially support Julia 1.4 and Julia
1.5.</p>
]]></content:encoded></item><item><title>Intel oneAPI</title><link>https://juliagpu.org/oneapi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliagpu.org/oneapi/</guid><description>oneAPI is an open standard for programming hardware accelerators, originally designed by Intel. The oneAPI.jl package offers a Julia interface to this programming model. The package is in early development, but already provides most features for application development.
Similarly to other GPU support packages in Julia, oneAPI.jl makes it possible to work with accelerators at three distinct abstraction levels:
high-level, using the oneArray array type and Julia&amp;rsquo;s powerful array abstractions; by writing your own kernels and launching them using the @oneapi macro; using the low-level Level Zero wrappers in the oneL0 submodule.</description><content:encoded><![CDATA[<p><a href="https://github.com/JuliaGPU/oneAPI.jl"><img src="https://img.shields.io/github/stars/JuliaGPU/oneAPI.jl?style=social" alt=""></a></p>
<p>oneAPI is an open standard for programming hardware accelerators, originally designed by
Intel. The <a href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a> package offers a Julia
interface to this programming model. The package is in early development, but already
provides most features for application development.</p>
<p>Similarly to other GPU support packages in Julia, oneAPI.jl makes it possible to work with
accelerators at three distinct abstraction levels:</p>
<ul>
<li>high-level, using the <code>oneArray</code> array type and Julia&rsquo;s powerful array abstractions;</li>
<li>by writing your own kernels and launching them using the <code>@oneapi</code> macro;</li>
<li>using the low-level Level Zero wrappers in the <code>oneL0</code> submodule.</li>
</ul>
<p>For more information, refer to the <a href="https://juliagpu.org/2020-11-05-oneapi_0.1/">introductory blog post</a>.</p>
]]></content:encoded></item><item><title>Learn</title><link>https://juliagpu.org/learn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliagpu.org/learn/</guid><description>Currently, the Julia CUDA stack is the most mature, easiest to install, and full-featured. The CUDA.jl repository is a central place for the documentation of all relevant packages. Start with the instructions on how to install the stack, and follow with this introductory tutorial.
If you prefer videos, the presentations below highlight different aspects of the toolchain.
Effective CUDA GPU computing in Julia Design and benefits of the Julia GPU stack Composability with existing (non-GPU) software Performance killers and tools for optimization Demonstration How Julia is compiled to CUDA GPUs Design and implementation of the Julia language Retargeting the language to GPUs Use of LLVM with LLVM.</description><content:encoded><![CDATA[<p>Currently, the Julia CUDA stack is the most mature, easiest to install, and
full-featured. The <a href="https://juliagpu.gitlab.io/CUDA.jl/">CUDA.jl</a> repository is
a central place for the documentation of all relevant packages. Start with the
instructions on <a href="https://juliagpu.gitlab.io/CUDA.jl/installation/overview/">how to
install</a> the stack,
and follow with this <a href="https://juliagpu.gitlab.io/CUDA.jl/tutorials/introduction/">introductory
tutorial</a>.</p>
<p>If you prefer videos, the presentations below highlight different aspects
of the toolchain.</p>
<h2 id="effective-cuda-gpu-computing-in-julia">Effective CUDA GPU computing in Julia</h2>
<ul>
<li>Design and benefits of the Julia GPU stack</li>
<li>Composability with existing (non-GPU) software</li>
<li>Performance killers and tools for optimization</li>
<li>Demonstration</li>
</ul>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/7Yq1UyncDNc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

</br>
<h2 id="how-julia-is-compiled-to-cuda-gpus">How Julia is compiled to CUDA GPUs</h2>
<ul>
<li>Design and implementation of the Julia language</li>
<li>Retargeting the language to GPUs</li>
<li>Use of LLVM with LLVM.jl</li>
<li>Benefits of a high-level language for GPU programming</li>
</ul>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/Fz-ogmASMAE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

]]></content:encoded></item><item><title>NVIDIA CUDA</title><link>https://juliagpu.org/cuda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliagpu.org/cuda/</guid><description>The programming support for NVIDIA GPUs in Julia is provided by the CUDA.jl package. It is built on the CUDA toolkit, and aims to be as full-featured and offer the same performance as CUDA C. The toolchain is mature, has been under development since 2014 and can easily be installed on any current version of Julia using the integrated package manager.
CUDA.jl makes it possible to program NVIDIA GPUs at different abstraction levels:</description><content:encoded><![CDATA[<p><a href="https://juliagpu.gitlab.io/CUDA.jl/"><img src="https://img.shields.io/badge/docs-latest-blue.svg" alt=""></a> <a href="https://github.com/JuliaGPU/CUDA.jl"><img src="https://img.shields.io/github/stars/JuliaGPU/CUDA.jl?style=social" alt=""></a></p>
<p>The programming support for NVIDIA GPUs in Julia is provided by the
<a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> package. It is built on the CUDA toolkit, and
aims to be as full-featured and offer the same performance as CUDA C. The toolchain is
mature, has been under development since 2014 and can easily be installed on any current
version of Julia using the integrated package manager.</p>
<p>CUDA.jl makes it possible to program NVIDIA GPUs at different abstraction levels:</p>
<ul>
<li>by using the <code>CuArray</code> type, providing a user-friendly yet powerful abstraction that does
not require any GPU programming experience;</li>
<li>by writing CUDA kernels, with the same performance as kernels written in CUDA C;</li>
<li>by interfacing with CUDA APIs and libraries directly, offering the same level of
flexibility you would expect from a C-based programming environment.</li>
</ul>
<p>The <a href="https://juliagpu.gitlab.io/CUDA.jl/">documentation</a> of CUDA.jl demonstrates each of
these approaches.</p>
<h2 id="performance">Performance</h2>
<p>Julia on the CPU is known for its good performance, approaching that of statically compiled
languages like C. The same holds for programming NVIDIA GPUs with kernels written using
CUDA.jl, where we have <a href="https://www.sciencedirect.com/science/article/pii/S0965997818310123">shown</a> the performance to approach and even
sometimes exceed that of CUDA C on a selection<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> of applications from the Rodinia
benchmark suite:</p>




  


<div class="card mb-3" style="">
  <a href="https://juliagpu.org/cuda/performance.png">
    <img src="https://juliagpu.org/cuda/performance_hua6459c56873bf72c41d6d1d8143eaa73_342008_700x0_resize_box_2.png" class="card-img-top" alt="">
  </a>
  
    <div class="card-body">
    
    
      <p class="card-text">Relative performance of Rodinia benchmarks <a href="https://github.com/JuliaParallel/rodinia">implemented in Julia with CUDA.jl</a>.</p>
    
    </div>
  
</div>
<h2 id="publications">Publications</h2>
<p>Much of the software in this toolchain was developed as part of academic research. If you
would like to help support it, please star the relevant repositories as such metrics may
help us secure funding in the future. If you use our software as part of your research,
teaching, or other activities, we would be grateful if you could cite our work:</p>
<ul>
<li>
<p>Tim Besard, Valentin Churavy, Alan Edelman and Bjorn De Sutter. &ldquo;<a href="https://www.sciencedirect.com/science/article/pii/S0965997818310123">Rapid software
prototyping for heterogeneous and distributed platforms.</a>&rdquo; <em>Advances in
Engineering Software</em> (2019).</p>
</li>
<li>
<p>Tim Besard, Christophe Foket, and Bjorn De Sutter. &ldquo;<a href="https://ieeexplore.ieee.org/abstract/document/8471188">Effective extensible programming:
Unleashing Julia on GPUs.</a>&rdquo; <em>IEEE
Transactions on Parallel and Distributed Systems</em> (2018).</p>
</li>
<li>
<p>Tim Besard. &ldquo;<a href="https://blog.maleadt.net/phd.pdf">Abstractions for Programming Graphics Processors in High-Level Programming
Languages.</a>&rdquo; (2019) PhD dissertation.</p>
</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Since porting applications from one programming language to another is labour
intensive, we only ported and analyzed the 10 smallest benchmarks from the suite. More
details can be found in <a href="https://www.sciencedirect.com/science/article/pii/S0965997818310123">the paper</a>. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title>Other</title><link>https://juliagpu.org/other/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliagpu.org/other/</guid><description>Several other back-ends exist, not all of them with the same level of polish or support as the NVIDIA and AMD back-ends.
OpenCL Programming OpenCL GPUs in Julia is much more limited than other supported platforms. On recent versions of Julia, only OpenCL.jl is available. This package can be used to compile and execute GPU kernels written in OpenCL C.
ArrayFire ArrayFire is a general-purpose software library that targets CPUs, GPUs, and other accelerator hardware.</description><content:encoded><![CDATA[<p>Several other back-ends exist, not all of them with the same level of polish or
support as the NVIDIA and AMD back-ends.</p>
<h2 id="opencl">OpenCL</h2>
<p>Programming OpenCL GPUs in Julia is much more limited than other supported platforms. On
recent versions of Julia, only <a href="https://github.com/JuliaGPU/OpenCL.jl">OpenCL.jl</a> is
available. This package can be used to compile and execute GPU kernels written in OpenCL C.</p>
<h2 id="arrayfire">ArrayFire</h2>
<p>ArrayFire is a general-purpose software library that targets CPUs, GPUs, and other
accelerator hardware. The <a href="https://github.com/JuliaGPU/ArrayFire.jl">ArrayFire.jl</a> package
provides a Julia interface to this library, and makes it possible to program accelerators
using an array abstraction built on the ArrayFire library.</p>
]]></content:encoded></item></channel></rss>